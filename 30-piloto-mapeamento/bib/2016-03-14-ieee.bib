@INPROCEEDINGS{6391736,
author={A. L. Schwerz and R. Liberato and I. S. Wiese and I. Steinmacher
and M. A. Gerosa and J. E. Ferreira},
booktitle={Collaborative Systems (SBSC), 2012 Brazilian Symposium on},
title={Prediction of Developer Participation in Issues of Open Source
Projects},
year={2012},
pages={109-114},
abstract={Developers of distributed open source projects use management
and issues tracking tool to communicate. These tools provide a large
volume of unstructured information that makes the triage of issues
difficult, increasing developers' overhead. This problem is common to
online communities based on volunteer participation. This paper shows
the importance of the content of comments in an open source project to
build a classifier to predict the participation for a developer in an
issue. To design this prediction model, we used two machine learning
algorithms called Naive Bayes and J48. We used the data of three Apache
Hadoop subprojects to evaluate the use of the algorithms. By applying
our approach to the most active developers of these subprojects we have
achieved an accuracy ranging from 79% to 96%. The results indicate that
the content of comments in issues of open source projects is a relevant
factor to build a classifier of issues for developers.},
keywords={learning (artificial intelligence);public domain
software;Apache Hadoop subprojects;developer participation;distributed
open source projects;machine learning algorithm;naive Bayes;prediction
model;tracking tool;unstructured information;volunteer
participation;Argon;Bayesian methods;Electronic mail;Machine learning
algorithms;Predictive models;Random access memory;Software;Content
analysis;issue tracking classifier;machine learning;prediction model},
doi={10.1109/SBSC.2012.27},
month={Oct},}
@INPROCEEDINGS{5741332,
author={A. Lamkanfi and S. Demeyer and Q. D. Soetens and T. Verdonck},
booktitle={Software Maintenance and Reengineering (CSMR), 2011 15th
European Conference on},
title={Comparing Mining Algorithms for Predicting the Severity of a
Reported Bug},
year={2011},
pages={249-258},
abstract={A critical item of a bug report is the so-called "severity",
i.e. the impact the bug has on the successful execution of the software
system. Consequently, tool support for the person reporting the bug in
the form of a recommender or verification system is desirable. In
previous work we made a first step towards such a tool: we demonstrated
that text mining can predict the severity of a given bug report with a
reasonable accuracy given a training set of sufficient size. In this
paper we report on a follow-up study where we compare four well-known
text mining algorithms (namely, Naive Bayes, Naive Bayes Multinomial,
K-Nearest Neighbor and Support Vector Machines) with respect to accuracy
and training set size. We discovered that for the cases under
investigation (two open source systems: Eclipse and GNOME) Naive Bayes
Multinomial performs superior compared to the other proposed algorithms.},
keywords={data mining;program debugging;program verification;text
analysis;Eclipse;GNOME;K-nearest neighbor;bug report;mining
algorithm;naive Bayes multinomial;software system;support vector
machines;text mining;verification system;Accuracy;Computer
bugs;Prediction algorithms;Software;Text mining;Training;Bug Reports;Bug
Severity;Bugzilla;Naive Bayes;Text Mining},
doi={10.1109/CSMR.2011.31},
ISSN={1534-5351},
month={March},}
@INPROCEEDINGS{6747219,
author={L. Meurice and A. Cleve},
booktitle={Software Maintenance, Reengineering and Reverse Engineering
(CSMR-WCRE), 2014 Software Evolution Week - IEEE Conference on},
title={DAHLIA: A visual analyzer of database schema evolution},
year={2014},
pages={464-468},
abstract={In a continuously changing environment, software evolution
becomes an unavoidable activity. The mining software repositories (MSR)
field studies the valuable data available in software repositories such
as source code version-control systems, issue/bug-tracking systems, or
communication archives. In recent years, many researchers have used MSR
techniques as a way to support software understanding and evolution.
While many software systems are data-intensive, i.e., their central
artefact is a database, little attention has been devoted to the
analysis of this important system component in the context of software
evolution. The goal of our work is to reduce this gap by considering the
database evolution history as an additional information source to aid
software evolution. We present DAHLIA (Database ScHema EvoLutIon
Analysis), a visual analyzer of database schema evolution. Our tool
mines the database schema evolution history from the software repository
and allows its interactive, visual analysis. We describe DAHLIA and
present our novel approach supporting data-intensive software evolution.},
keywords={data mining;data visualisation;database management
systems;program debugging;software maintenance;source code
(software);DAHLIA;MSR techniques;bug tracking systems;communication
archives;data-intensive software evolution;database schema
evolution;issue tracking systems;mining software repositories
field;software understanding;source code version-control systems;visual
analyzer;History;Indexes;Software systems;Visual databases;Visualization},
doi={10.1109/CSMR-WCRE.2014.6747219},
month={Feb},}
@INPROCEEDINGS{6861074,
author={M. El Hamlaoui and S. Ebersold and B. Coulette and M. Nassar and
A. Anwar},
booktitle={Research Challenges in Information Science (RCIS), 2014 IEEE
Eighth International Conference on},
title={Heterogeneous models matching for consistency management},
year={2014},
pages={1-12},
abstract={This work is situated in the context of the application of
Model Driven Engineering to complex systems view-based modelling. In
fact, view-based models - called also partial models - are manipulated
by different actors (designers), and are thus generally heterogeneous,
that is, described with different DSLs (Domain Specific Languages).
Instead of building a single global model, which is not realistic, we
propose to organize the different partial models as a network of related
models, which provides a global view of the system through a
correspondence model. As models are modelled separately by different
designers, they also evolve separately that induces a problem of
consistency. To solve it, we propose a semi-automatic process based on
the correspondence model allowing detecting changes, calculating their
impacts, and proposing modifications to maintain the consistency among
them. The approach is supported by a tool chain and illustrated by the
example of a Bug Tracking System.},
keywords={software engineering;Domain Specific Languages;consistency
management;correspondence model;heterogeneous model matching;model
driven engineering;Abstracts;Adaptation models;Analytical
models;Business;DSL;Information systems;Unified modeling
language;Heterogeneous models;change
processing;consistency;correspondence model},
doi={10.1109/RCIS.2014.6861074},
month={May},}
@ARTICLE{4721180,
author={H. C. Gall and B. Fluri and M. Pinzger},
journal={IEEE Software},
title={Change Analysis with Evolizer and ChangeDistiller},
year={2009},
volume={26},
number={1},
pages={26-33},
abstract={Evolizer a platform for mining software archives, and
ChangeDistiller, a change extraction and analysis tool, enable the
retrospective analysis of a software system's evolution.},
keywords={configuration management;management of change;software
development management;software
prototyping;ChangeDistiller;Evolizer;change analysis;fine-grained change
type analysis;fine-grained change type extraction;integrated development
environment;issue-tracking systems;software archives;software evolution
analysis;software systems;source code version-control systems;technology
requirements;Costs;Data mining;Feedback;History;Information
analysis;Java;Project management;Software systems;Software
tools;Taxonomy;data mining;enhancement;maintenance;recommender
systems;software evolution analysis;version control},
doi={10.1109/MS.2009.6},
ISSN={0740-7459},
month={Jan},}
@INPROCEEDINGS{6018025,
author={J. Clarke and J. Vines and K. Kirk and E. Mark and R. Angelini
and C. Spear and N. Waisbrot and J. Martin and K. Leiter and B. Hoffman},
booktitle={High Performance Computing Modernization Program Users Group
Conference (HPCMP-UGC), 2010 DoD},
title={A Common Computational Science Environment for High Performance
Computing Centers},
year={2010},
pages={442-449},
abstract={The Computational Sciences Environment (CSE) was developed to
provide a standard development platform for data analysis,
visualization, and software testing and evaluation. The Classified Data
Analysis and Assessment Center (CDAAC), in conjunction with the
Computational Sciences and Engineering Branch (CSEB) of the Army
Research Laboratory, assembled a set of open source data analysis tools
and applications, software management and testing tools, and libraries
necessary to run the tools; and made them available as a package called
CSE. CSE also provides experimental software builds for users who might
need newer features than what is currently available in the release
package. The CSE team provides support for developers, the end-user, and
distributed development teams. Tests are regularly run on the software,
both current and release, and the results are submitted to quality
dashboards for review using CTest. CTest is part of an open-source
software building tool called CMake. Developers can use CSE as a
template and can customize it to meet their specific project goals. CSE
provides developers with a common tool set to assist in developing
portable high performance computing (HPC) Applications. Development of
CSE has been managed through an open-source project management
application that provides Software Configuration Management (SCM)
integration of the CSE repository, informational wikis, bug tracking,
and feature requests. To support distributed development teams, CSE
provides project management tools, software repositories, SCM, and
online software quality dashboards.},
keywords={computer centres;data analysis;data visualisation;program
testing;project management;public domain software;software
libraries;software management;software packages;software
quality;software tools;CSE;CTest;SCM;classified data analysis and
assessment center;computational science environment;data
visualization;high performance computing centre;informational
wikis;libraries;open source software;project management tools;quality
dashboards;software configuration management;software package;software
quality;software repositories;software testing;software
tools;Buildings;Data analysis;Data
visualization;Libraries;Software;Testing;US Department of Defense},
doi={10.1109/HPCMP-UGC.2010.68},
month={June},}
@INPROCEEDINGS{6976066,
author={D. Huo and T. Ding and C. McMillan and M. Gethers},
booktitle={Software Maintenance and Evolution (ICSME), 2014 IEEE
International Conference on},
title={An Empirical Study of the Effects of Expert Knowledge on Bug
Reports},
year={2014},
pages={1-10},
abstract={Bug reports are crucial software artifacts for both software
maintenance researchers and practitioners. A typical use of bug reports
by researchers is to evaluate automated software maintenance tools: a
large repository of reports is used as input for a tool, and metrics are
calculated from the tool's output. But this process is quite different
from practitioners, who distinguish between reports written by experts
such as programmers, and reports written by non-experts such as users.
Practitioners recognize that the content of a bug report depends on its
author's expert knowledge. In this paper, we present an empirical study
of the textual difference between bug reports written by experts and
non-experts. We find that a significance difference exists, and that
this difference has a significant impact on the results from a
state-of-the-art feature location tool. Our recommendation is that
researchers evaluate maintenance tools using different sets of bug
reports for experts and non-experts.},
keywords={program debugging;software maintenance;software
metrics;automated software maintenance tool evaluation;bug report
content;empirical analysis;expert knowledge;feature location
tool;large-report repository;software artifacts;software maintenance
practitioners;software maintenance researchers;software metrics;textual
difference;Computer bugs;Information retrieval;Maintenance
engineering;Measurement;Semantics;Software maintenance;bug
reports;empirical study;expert knowledge},
doi={10.1109/ICSME.2014.22},
ISSN={1063-6773},
month={Sept},}
@INPROCEEDINGS{6982639,
author={T. D. B. Le and F. Thung and D. Lo},
booktitle={Software Reliability Engineering (ISSRE), 2014 IEEE 25th
International Symposium on},
title={Predicting Effectiveness of IR-Based Bug Localization Techniques},
year={2014},
pages={335-345},
abstract={Recently, many information retrieval (IR) based bug
localization approaches have been proposed in the literature. These
approaches use information retrieval techniques to process a textual bug
report and a collection of source code files to find buggy files. They
output a ranked list of files sorted by their likelihood to contain the
bug. Recent approaches can achieve reasonable accuracy, however, even a
state-of-the-art bug localization tool outputs many ranked lists where
buggy files appear very low in the lists. This potentially causes
developers to distrust bug localization tools. Parnin and Orso recently
conduct a user study and highlight that developers do not find an
automated debugging tool useful if they do not find the root cause of a
bug early in a ranked list. To address this problem, we build an oracle
that can automatically predict whether a ranked list produced by an
IR-based bug localization tool is likely to be effective or not. We
consider a ranked list to be effective if a buggy file appears in the
top-N position of the list. If a ranked list is unlikely to be
effective, developers do not need to waste time in checking the
recommended files one by one. In such cases, it is better for developers
to use traditional debugging methods or request for further information
to localize bugs. To build this oracle, our approach extracts features
that can be divided into four categories: score features, textual
features, topic model features, and metadata features. We build a
separate prediction model for each category, and combine them to create
a composite prediction model which is used as the oracle. We name our
proposed approach APRILE, which stands for Automated Prediction of
IR-based Bug Localization's Effectiveness. We have evaluated APRILE to
predict the effectiveness of three state-of-the-art IR based bug
localization tools on more than three thousands bug reports from
AspectJ, Eclipse, and SWT. APRILE can achieve an average precision,
recall, and - -measure of at least 70.36%, 66.94%, and 68.03%,
respectively. Furthermore, APRILE outperforms a baseline approach by
84.48%, 17.74%, and 31.56% for the AspectJ, Eclipse, and SWT bug
reports, respectively.},
keywords={feature extraction;information retrieval;meta data;program
debugging;source code (software);APRILE;AspectJ;Eclipse;IR-based bug
localization techniques;SWT;automated debugging tool;automated
prediction of IR-based bug localization effectiveness;bug localization
tool;buggy files;composite prediction model;feature
extraction;information retrieval based bug localization
approach;information retrieval techniques;metadata features;recommended
files;score features;source code files;textual bug report;textual
features;topic model features;traditional debugging method;Computational
modeling;Computer bugs;Debugging;Feature extraction;Predictive
models;Support vector machines;Training;Bug Localization;Bug
Reports;Effectiveness Prediction;Information Retrieval;Text
Classification},
doi={10.1109/ISSRE.2014.39},
ISSN={1071-9458},
month={Nov},}
@INPROCEEDINGS{4814136,
author={M. A. Storey and J. Ryall and R. I. Bull and D. Myers and J.
Singer},
booktitle={Software Engineering, 2008. ICSE '08. ACM/IEEE 30th
International Conference on},
title={TODO or to bug},
year={2008},
pages={251-260},
abstract={Software development is a highly collaborative activity that
requires teams of developers to continually manage and coordinate their
programming tasks. In this paper, we describe an empirical study that
explored how task annotations embedded within the source code play a
role in how software developers manage personal and team tasks. We
present findings gathered by combining results from a survey of
professional software developers, an analysis of code from open source
projects, and interviews with software developers. Our findings help us
describe how task annotations can be used to support a variety of
activities fundamental to articulation work within software development.
We describe how task management is negotiated between the more formal
issue tracking systems and the informal annotations that programmers
write within their source code. We report that annotations have
different meanings and are dependent on individual, team and community
use. We also present a number of issues related to managing annotations,
which may have negative implications for maintenance. We conclude with
insights into how these findings could be used to improve tool support
and software process.},
keywords={groupware;program debugging;public domain software;task
analysis;TODO;To Bug;collaborative activity;open source
projects;software developers;task annotations;task
management;Collaborative software;Collaborative tools;Collaborative
work;Embedded software;Open source software;Programming
profession;Project management;Protocols;Software development
management;Software tools;source code comments;task annotations;work
practices},
doi={10.1145/1368088.1368123},
ISSN={0270-5257},
month={May},}
@INPROCEEDINGS{4343777,
author={M. P. Francisco and P. B. Perez and G. Robles},
booktitle={Empirical Software Engineering and Measurement, 2007. ESEM
2007. First International Symposium on},
title={Correlation between bug notifications, messages and participants
in Debian's bug tracking system},
year={2007},
pages={455-457},
abstract={Bugs are an essential part of software projects because they
lead its evolution. Without bug notifications developers cannot know if
their software is accomplishing its tasks properly. However, few
analytical studies have been made about this aspect of projects. We have
developed a tool to extract and to store information from Debian's BTS
(Bug Tracking System) in a relational database. In this paper we show
that there is a strong dependence between three variables which can be
used to analyze the activity of a project through its bugs: bug
notifications, communications between users and developers and people
involved.},
keywords={program debugging;relational databases;Debian bug tracking
system;bug notifications;relational database;software projects;Computer
bugs;Control systems;Data mining;Information
retrieval;Internet;Packaging;Particle measurements;Relational
databases;Software engineering;Software measurement},
doi={10.1109/ESEM.2007.65},
ISSN={1938-6451},
month={Sept},}
@INPROCEEDINGS{5071004,
author={En Ye and L. A. Neiman and H. Q. Dinh and C. Liu},
booktitle={Software Engineering - Companion Volume, 2009. ICSE-Companion
2009. 31st International Conference on},
title={SecondWATCH: A workspace awareness tool based on a 3-D virtual
world},
year={2009},
pages={291-294},
abstract={Awareness of fellow developers' activities has been widely
recognized as essential in facilitating collaboration in a software
developing team. However, as reported in several field studies on
software development, awareness information on software artifact and
coworker is difficult to acquire. To help software developers maintain
group awareness and enhance their collaboration, we developed a
prototype workspace awareness tool called SecondWATCH based on Second
Life, a 3D online virtual world. SecondWATCH informs developers of
real-time and history artifact and coworker information by monitoring
team members' activities on their local workspaces, version control
repository, and bug tracking system. It then extracts, analyzes, and
visualizes the information in SL as a common view shared by the whole
team using a 3-D city metaphor. We have successfully used SecondWATCH to
visualize history information of three open-source Java projects,
Free-Mind, JEdit, and GUJ, and also using it as our workspace awareness
tool when we are developing it.},
keywords={avatars;configuration management;groupware;program
debugging;software development management;3-D online virtual
world;SecondWATCH;avatar;bug tracking system;collaborative work;coworker
information;group awareness;software artifact;software development
team;version control repository;workspace awareness tool;Collaborative
software;Collaborative tools;Collaborative work;Fellows;History;Online
Communities/Technical Collaboration;Programming;Software
maintenance;Software tools;Visualization},
doi={10.1109/ICSE-COMPANION.2009.5071004},
month={May},}
@INPROCEEDINGS{7320414,
author={W. Maalej and H. Nabil},
booktitle={Requirements Engineering Conference (RE), 2015 IEEE 23rd
International},
title={Bug report, feature request, or simply praise? On automatically
classifying app reviews},
year={2015},
pages={116-125},
abstract={App stores like Google Play and Apple AppStore have over 3
Million apps covering nearly every kind of software and service.
Billions of users regularly download, use, and review these apps. Recent
studies have shown that reviews written by the users represent a rich
source of information for the app vendors and the developers, as they
include information about bugs, ideas for new features, or documentation
of released features. This paper introduces several probabilistic
techniques to classify app reviews into four types: bug reports, feature
requests, user experiences, and ratings. For this we use review metadata
such as the star rating and the tense, as well as, text classification,
natural language processing, and sentiment analysis techniques. We
conducted a series of experiments to compare the accuracy of the
techniques and compared them with simple string matching. We found that
metadata alone results in a poor classification accuracy. When combined
with natural language processing, the classification precision got
between 70-95% while the recall between 80-90%. Multiple binary
classifiers outperformed single multiclass classifiers. Our results
impact the design of review analytics tools which help app vendors,
developers, and users to deal with the large amount of reviews, filter
critical reviews, and assign them to the appropriate stakeholders.},
keywords={data mining;natural language processing;pattern
classification;probability;software reviews;string matching;text
analysis;App stores;Apple AppStore;Google Play;app reviews
classification;bug report;feature requests;multiclass
classifiers;multiple binary classifiers;natural language
processing;review analytics tool design;review metadata;sentiment
analysis techniques;several probabilistic techniques;star rating;string
matching;text classification;user experiences;Accuracy;Computer
crashes;Google;Machine learning algorithms;Metadata;Natural language
processing;Training},
doi={10.1109/RE.2015.7320414},
month={Aug},}
@INPROCEEDINGS{7181432,
author={M. White and M. Linares-Vasquez and P. Johnson and C.
Bernal-Cardenas and D. Poshyvanyk},
booktitle={Program Comprehension (ICPC), 2015 IEEE 23rd International
Conference on},
title={Generating Reproducible and Replayable Bug Reports from Android
Application Crashes},
year={2015},
pages={48-59},
abstract={Manually reproducing bugs is time-consuming and tedious.
Software maintainers routinely try to reproduce unconfirmed issues using
incomplete or no informative bug reports. Consequently, while
reproducing an issue, the maintainer must augment the report with
information - such as a reliable sequence of descriptive steps to
reproduce the bug - to aid developers with diagnosing the issue. This
process encumbers issue resolution from the time the bug is entered in
the issue tracking system until it is reproduced. This paper presents
Crash Droid, an approach for automating the process of reproducing a bug
by translating the call stack from a crash report into expressive steps
to reproduce the bug and a kernel event trace that can be replayed
on-demand. Crash Droid manages trace ability links between scenarios'
natural language descriptions, method call traces, and kernel event
traces. We evaluated Crash Droid on several open-source Android
applications infected with errors. Given call stacks from crash reports,
Crash Droid was able to generate expressive steps to reproduce the bugs
and automatically replay the crashes. Moreover, users were able to
confirm the crashes faster with Crash Droid than manually reproducing
the bugs or using a stress-testing tool.},
keywords={Android (operating system);program debugging;software
maintenance;Android application crashes;CRASHDROID approach;bug
diagnosis;call stack;crash report;incomplete-noninformative bug
reports;issue tracking system;kernel event trace;kernel event
traces;method call traces;natural language descriptions;open-source
Android applications;reproducible replayable bug report
generation;stress-testing tool;traceability links;Androids;Computer
crashes;Databases;Humanoid robots;Kernel;Mobile communication;Natural
languages;Android;crash and bug reports;reproducibility},
doi={10.1109/ICPC.2015.14},
month={May},}
@INPROCEEDINGS{5959723,
author={G. Ghezzi and H. C. Gall},
booktitle={Software Architecture (WICSA), 2011 9th Working IEEE/IFIP
Conference on},
title={SOFAS: A Lightweight Architecture for Software Analysis as a
Service},
year={2011},
pages={93-102},
abstract={Access to data stored in software repositories by systems such
as version control, bug and issue tracking, or mailing lists is
essential for assessing the quality of a software system. A myriad of
analyses exploiting that data have been proposed throughout the years:
source code analysis, code duplication analysis, co-change analysis, bug
prediction, or detection of bug fixing patterns. However, easy and
straight forward synergies between these analyses rarely exist. To
tackle this problem we have developed SOFAS, a distributed and
collaborative software analysis platform to enable a seamless
interoperation of such analyses. In particular, software analyses are
offered as Restful web services that can be accessed and composed over
the Internet. SOFAS services are accessible through a software analysis
catalog where any project stakeholder can, depending on the needs or
interests, pick specific analyses, combine them, let them run remotely
and then fetch the final results. That way, software developers,
testers, architects, or quality assurance experts are given access to
quality analysis services. They are shielded from many peculiarities of
tool installations and configurations, but SOFAS offers them
sophisticated and easy-to-use analyses. This paper describes in detail
our SOFAS architecture, its considerations and implementation aspects,
and the current set of implemented and offered Restful analysis services.},
keywords={Web services;groupware;information retrieval;program
testing;service-oriented architecture;software quality;Internet;RESTful
Web service;SOFAS;bug fixing pattern;bug prediction;cochange
analysis;code duplication analysis;collaborative software analysis
platform;data access;issue tracking;lightweight architecture;mailing
list;quality analysis service;quality assurance expert;software analysis
as a service;software developer;software repository;source code
analysis;version control;Computer
architecture;Couplings;History;Measurement;Ontologies;Software;Web
services},
doi={10.1109/WICSA.2011.21},
month={June},}
@INPROCEEDINGS{6899207,
author={X. Xia and D. Lo and W. Qiu and X. Wang and B. Zhou},
booktitle={Computer Software and Applications Conference (COMPSAC), 2014
IEEE 38th Annual},
title={Automated Configuration Bug Report Prediction Using Text Mining},
year={2014},
pages={107-116},
abstract={Configuration bugs are one of the dominant causes of software
failures. Previous studies show that a configuration bug could cause
huge financial losses in a software system. The importance of
configuration bugs has attracted various research studies, e.g., To
detect, diagnose, and fix configuration bugs. Given a bug report, an
approach that can identify whether the bug is a configuration bug could
help developers reduce debugging effort. We refer to this problem as
configuration bug reports prediction. To address this problem, we
develop a new automated framework that applies text mining technologies
on the natural-language description of bug reports to train a
statistical model on historical bug reports with known labels (i.e.,
Configuration or non-configuration), and the statistical model is then
used to predict a label for a new bug report. Developers could apply our
model to automatically predict labels of bug reports to improve their
productivity. Our tool first applies feature selection techniques (e.g.,
Information gain and Chi-square) to pre-process the textual information
in bug reports, and then applies various text mining techniques (e.g.,
Naive Bayes, SVM, naive Bayes multinomial) to build statistical models.
We evaluate our solution on 5 bug report datasets including accumulo,
activemq, camel, flume, and wicket. We show that naive Bayes multinomial
with information gain achieves the best performance. On average across
the 5 projects, its accuracy, configuration F-measure and
non-configuration F-measure are 0.811, 0.450, and 0.880, respectively.
We also compare our solution with the method proposed by Arshad et al.
The results show that our proposed approach that uses naive Bayes
multinomial with information gain on average improves accuracy,
configuration F-measure and non-configuration F-measure scores of Arshad
et al.'s method by 8.34%, 103.7%, and 4.24%, respectively.},
keywords={data mining;program debugging;statistical analysis;text
analysis;accumulo;activemq;bug detection;bug
diagnosis;camel;configuration F-measure;configuration bug report
prediction;debugging effort;feature selection
techniques;flume;information gain;naive Bayes
multinomial;natural-language description;software failure;statistical
model;text mining;wicket;Buildings;Computer bugs;Feature
extraction;Predictive models;Support vector machines;Text
mining;Training;Configuration Bug;Data Mining;Feature Selection},
doi={10.1109/COMPSAC.2014.17},
month={July},}
@INPROCEEDINGS{6080839,
author={M. D'Ambros and R. Robbes},
booktitle={Software Maintenance (ICSM), 2011 27th IEEE International
Conference on},
title={Effective mining of software repositories},
year={2011},
pages={598-598},
abstract={With the advent of open-source, the Internet, and the
consequent widespread adoption of distributed development tools, such as
software configuration management and issue tracking systems, a vast
amount of valuable information concerning software development and
evolution has become available. Mining Software Repositories (MSR)-a
very active and interest-growing research field-deals with retrieving
and analyzing this data. Empirical analyses of software repositories
allow researchers to validate assumptions previously based only on
intuitions, as well as finding novel theories. In turn, these theories
about the software development phenomenon have been translated into
concrete approaches and tools that support software developers and
managers in their daily tasks. In this tutorial, we provide an overview
of the state of the art of MSR. In particular, we describe what software
repositories are, what in turn Mining Software Repositories is, what
techniques are available to researchers and practitioners, and finally,
what the limitations of MSR are nowadays, and how to fix them.},
keywords={configuration management;data mining;software
maintenance;Internet;MSR;distributed development tool;issue tracking
system;mining software repositories;software configuration
management;software development;software evolution},
doi={10.1109/ICSM.2011.6080839},
ISSN={1063-6773},
month={Sept},}
@INPROCEEDINGS{7081811,
author={Shihai Jiang and Liwei Shen and Xin Peng and Zhaojin Lv and
Wenyun Zhao},
booktitle={Software Analysis, Evolution and Reengineering (SANER), 2015
IEEE 22nd International Conference on},
title={Understanding developers' natural language queries with
interactive clarification},
year={2015},
pages={13-22},
abstract={When performing software maintenance tasks, developers often
need to understand a series of background knowledge based on information
distributed in different software repositories such as source codes,
version control systems and bug tracking systems. An effective way to
support developers to understand such knowledge is to provide an
integrated knowledge base and allow them to ask questions using natural
language. Existing approaches cannot well support natural language
questions that involve a series of conceptual relationships and are
phrased in a flexible way. In this paper, we propose an interactive
approach for understanding developers' natural language queries. The
approach can understand a developer's natural language questions phrased
in different ways by generating a set of ranked and human-readable
candidate questions and getting feedback from the developer. Based on
the candidate question confirmed by the developer, the approach can then
synthesize an answer by constructing and executing a structural query to
the knowledge base. We have implemented a tool following the proposed
approach and conducted a user study using the tool. The results show
that our approach can help developers get the desired answers more
easily and accurately.},
keywords={knowledge based systems;natural language processing;program
debugging;query processing;software maintenance;source code
(software);bug tracking systems;developers;human-readable candidate
questions;interactive clarification;knowledge base;natural language
queries;natural language questions;software maintenance tasks;software
repositories;source codes;structural query;version control
systems;Computer bugs;Knowledge based systems;Natural
languages;Navigation;OWL;Ontologies;Software},
doi={10.1109/SANER.2015.7081811},
month={March},}
@INPROCEEDINGS{6079869,
author={N. Kaushik and L. Tahvildari and M. Moore},
booktitle={Reverse Engineering (WCRE), 2011 18th Working Conference on},
title={Reconstructing Traceability between Bugs and Test Cases: An
Experimental Study},
year={2011},
pages={411-414},
abstract={In manual testing, testers typically follow the steps listed
in the bug report to verify whether a bug has been fixed or not.
Depending on time and availability of resources, a tester may execute
some additional test cases to ensure test coverage. In the case of
manual testing, the process of finding the most relevant manual test
cases to run is largely manual and involves tester expertise. From a
usability standpoint, the task of finding the most relevant test cases
is tedious as the tester typically has to switch between the defect
management tool and the test case management tool in order to search for
test cases relevant to the bug at hand. In this paper, we use IR
techniques to recover trace ability between bugs and test cases with the
aim of recommending test cases for bugs. We report on our experience of
recovering trace ability between bugs and test cases using techniques
such as Latent Semantic Indexing (LSI) and Latent Dirichlet Allocation
(LDA) through a small industrial case study.},
keywords={program diagnostics;program testing;IR techniques;bug
report;defect management tool;latent dirichlet allocation;latent
semantic indexing;manual testing;test case management tool;test
cases;traceability reconstruction;usability standpoint;Computer
bugs;Couplings;Indexing;Large scale
integration;Manuals;Semantics;Testing;LSI;bug;test case;traceability},
doi={10.1109/WCRE.2011.58},
ISSN={1095-1350},
month={Oct},}
@INPROCEEDINGS{6606582,
author={J. Bell and N. Sarda and G. Kaiser},
booktitle={Software Engineering (ICSE), 2013 35th International
Conference on},
title={Chronicler: Lightweight recording to reproduce field failures},
year={2013},
pages={362-371},
abstract={When programs fail in the field, developers are often left
with limited information to diagnose the failure. Automated error
reporting tools can assist in bug report generation but without precise
steps from the end user it is often difficult for developers to recreate
the failure. Advanced remote debugging tools aim to capture sufficient
information from field executions to recreate failures in the lab but
often have too much overhead to practically deploy. We present
Chronicler, an approach to remote debugging that captures
non-deterministic inputs to applications in a lightweight manner,
assuring faithful reproduction of client executions. We evaluated
Chronicler by creating a Java implementation, ChroniclerJ, and then by
using a set of benchmarks mimicking real world applications and
workloads, showing its runtime overhead to be under 10% in most cases
(worst case 86%), while an existing tool showed overhead over 100% in
the same cases (worst case 2,322%).},
keywords={Java;program debugging;software fault tolerance;software
tools;CHRONICLERJ;Java implementation;advanced remote debugging
tools;automated error reporting tools;bug report generation;failure
diagnosis;field failures;nondeterministic inputs;Computer
crashes;Debugging;Instruction
sets;Instruments;Java;Libraries;Runtime;Debugging aids;Error handling
and recovery;Maintainability;Software maintenance},
doi={10.1109/ICSE.2013.6606582},
month={May},}
@INPROCEEDINGS{6688891,
author={S. Davies and M. Roper},
booktitle={Software Reliability Engineering Workshops (ISSREW), 2013
IEEE International Symposium on},
title={Bug localisation through diverse sources of information},
year={2013},
pages={126-131},
abstract={Many approaches have been proposed to address the problem of
bug localisation - taking a bug report and recommending to developers
the possible locations of the bug in the project. However, these can
often require significant up-front work from developers, and are not
widely adopted. Furthermore, those techniques which do not require this
up-front investment are often far from accurate, and do not take
advantage of all of the information that they could. We propose a
technique for combining information from multiple, novel sources of
information about a project and a bug, and use this to recommend bug
locations to developers. We also identify how this technique could be
used to create a low-effort tool for bug localisation, with the aim of
increasing developer adoption. We evaluate the technique on 1143 bugs in
three open-source projects, and find that it can be used to increase the
number of bugs where the first relevant method recommended to developers
is the top result from 98 to 132 and in the top-10 from 271 to 322.},
keywords={program debugging;project management;software management;bug
localisation;bug report;information sources;open-source
projects;up-front technique;Computer bugs;Computers;Data
models;History;Linear regression;Open source software;bug
localisation;mining software repositories;program debugging},
doi={10.1109/ISSREW.2013.6688891},
month={Nov},}
@INPROCEEDINGS{6143100,
author={M. C. Murray},
booktitle={Frontiers in Education Conference (FIE), 2011},
title={Work in progress #x2014; Creating a professional software
development environment to support capstone programming projects},
year={2011},
pages={S4F-1-S4F-3},
abstract={Experiential education provides valuable learning
opportunities for students in the computing disciplines. Assigning
students to work on real-world projects is often seen as a way for
students to practice what they have learned in the classroom. While a
desirable goal, logistics often make it difficult to provide these types
of experiences. However, it is vitally important for students to be
exposed to and experiment with tools used in commercial software
development environments. This paper provides a descriptive overview of
the development and implementation of a professional software
development environment used to support computer science capstone
programming projects. To date, the environment has been used to support
work on a National Science Foundation funded database courseware project
that includes over 100 interactive software modules. The environment was
built using a collection of open-source applications that provide
version control, task assignment and tracking, collaborative team tools,
bug tracking, and project documentation management. This project has
created a venue for providing consistent high quality real-world types
of experiences for students completing their capstone requirement.},
keywords={computer science education;courseware;database management
systems;public domain software;software engineering;bug
tracking;collaborative team tools;commercial software development
environments;computer science capstone programming projects;computing
disciplines;database courseware project;experiential
education;interactive software modules;national science foundation;open
source applications;professional software development
environment;project documentation management;students learning
opportunities;task assignment;task tracking;version
control;Courseware;Databases;Open source software;Programming;Capstone
projects;Experiential learning;Open-source software;Software development},
doi={10.1109/FIE.2011.6143100},
ISSN={0190-5848},
month={Oct},}
@INPROCEEDINGS{7332489,
author={C. S. Corley and F. Lois and S. Quezada},
booktitle={Software Maintenance and Evolution (ICSME), 2015 IEEE
International Conference on},
title={Web usage patterns of developers},
year={2015},
pages={381-390},
abstract={Developers often rely on the web-based tools for
troubleshooting, collaboration, issue tracking, code reviewing,
documentation viewing, and a myriad of other uses. Developers also use
the web for non-development purposes, such as reading news or social
media. In this paper we explore whether web usage is detriment to a
developer's focus on work from a sample over 150 developers.
Additionally, we investigate if highly-focused developers use the web
differently than other developers. Our qualitative findings suggest
highly-focused developers use the web differently, but we are unable to
predict a developer's focused based on web usage alone. Further
quantitative findings suggest that web usage does not have a negative
impact on a developer's focus.},
keywords={Internet;programming environments;software engineering;Web
usage pattern;Web-based tool;code reviewing;documentation
viewing;troubleshooting;Blogs;Buildings;Collaboration;Debugging;Encoding;Heating;Software;developer
focus;interruptions;personal software process;web activity},
doi={10.1109/ICSM.2015.7332489},
month={Sept},}
@INPROCEEDINGS{6062191,
author={J. Helming and M. Koegel},
booktitle={Software Engineering, 2010 ACM/IEEE 32nd International
Conference on},
title={Managing iterations with UNICASE},
year={2010},
volume={2},
pages={313-314},
abstract={Planning iterations in software projects requires considering
artifacts from different aspects such as requirements, specifications,
tasks or even bug reports. UNICASE is a unified CASE tool integrating
these relevant artifacts into one model. We demonstrate how the tool
supports planning and executing iterations.},
keywords={computer aided software engineering;formal
specification;program debugging;project management;software development
management;UNICASE;bug report;iteration management;iteration
planning;requirements;software project;specification;task;unified CASE
tool;Analytical models;Conferences;Planning;Software;Software
engineering;System analysis and design;Unified modeling language},
doi={10.1145/1810295.1810367},
ISSN={0270-5257},
month={May},}
@INPROCEEDINGS{6805386,
author={D. Correa and S. Lal and A. Saini and A. Sureka},
booktitle={Software Engineering Conference (APSEC), 2013 20th
Asia-Pacific},
title={Samekana: A Browser Extension for Including Relevant Web Links in
Issue Tracking System Discussion Forum},
year={2013},
volume={1},
pages={25-33},
abstract={Several widely used Issue tracking systems (such as Google
Issue Tracker and Bugzilla) contains an integrated threaded discussion
forum to facilitate discussion between the development and maintenance
team (bug reporters, bug triagers, bug fixers and quality assurance
managers). We observe that several comments (and even bug report
descriptions) posted to issue tracing system contains links to external
websites as references to knowledge sources relevant to the discussion.
We conduct a survey (and present the results of the survey) of Google
Chromium Developers on the importance and usefulness of web references
in issue tracking system comments and the need of a web-browser
extension which facilitates easy organization and inclusion of web-links
in the post. We conduct a characterization study on an experimental
dataset from Google Chromium Issue Tracking system and present results
on the distribution of number of links in the dataset, categorization of
links into pre-defined classes (such as blogs, community based Q&A
websites, developer discussion forums, version control system),
correlation of number and types of links with various bug report types
(such as security, crash, regression and clean-up) and relation between
presence of links and bug resolution time. Survey results and data
characterization study motivate the need of building a developer
productivity tool to facilitate web-link (as references) organization
and inclusion in issue tracking system comments. We present a Google
Chromium Web Browser Extension called as Samekana and publish the
extension on Google Chromium Web Store which can be freely downloaded by
users worldwide. The extension contains features such as annotating
(using tags, title and description) and saving web references pertaining
to multiple bug reports and tasks and then posting it as bibliography
(for easy citation and reference) in issue tracking system comments.},
keywords={Web sites;online front-ends;Bugzilla;Google chromium Web
browser extension;Google chromium Web store;Google chromium
developers;Google chromium issue tracking system;Google issue
tracker;Samekana;Web references;Web sites;bibliography;bug report
descriptions;bug report types;bug resolution time;building;data
characterization;including relevant Web links;issue tracing system;issue
tracking system comments;issue tracking system discussion
forum;maintenance team;productivity tool;Browsers;Chromium;Computer
crashes;Discussion forums;Google;Internet;Software;Developer
Productivity Tool;Empirical Software Engineering and Measurements
(ESEM);Mining Bug Reports;Mining Software Repositories (MSR);Software
Maintenance},
doi={10.1109/APSEC.2013.15},
ISSN={1530-1362},
month={Dec},}
@INPROCEEDINGS{6178935,
author={A. Hora and N. Anquetil and S. Ducasse and M. Bhatti and C.
Couto and M. T. Valente and J. Martins},
booktitle={Software Maintenance and Reengineering (CSMR), 2012 16th
European Conference on},
title={Bug Maps: A Tool for the Visual Exploration and Analysis of Bugs},
year={2012},
pages={523-526},
abstract={To harness the complexity of big legacy software, software
engineering tools need more and more information on these systems. This
information may come from analysis of the source code, study of
execution traces, computing of metrics, etc. One source of information
received less attention than source code: the bugs on the system. Little
is known about the evolutionary behavior, lifetime, distribution, and
stability of bugs. In this paper, we propose to consider bugs as first
class entities and a useful source of information that can answer such
topics. Such analysis is inherently complex, because bugs are
intangible, invisible, and difficult to be traced. Therefore, our tool
extracts information about bugs from bug tracking systems, link this
information to other software artifacts, and explore interactive
visualizations of bugs that we call bug maps.},
keywords={evolutionary computation;information resources;program
debugging;program testing;software maintenance;bug maps;bug tracking
system;bugs analysis;bugs stability;evolutionary behavior;information
source;interactive bugs visualization;legacy software;software
artifact;software engineering tools;source code;visual
exploration;Browsers;Color;Computer
bugs;History;Measurement;Software;Visualization},
doi={10.1109/CSMR.2012.68},
ISSN={1534-5351},
month={March},}
@INPROCEEDINGS{6363292,
author={H. Nakamura and R. Nagano and K. Hisazumi and Y. Kamei and N.
Ubayashi and A. Fukuda},
booktitle={Empirical Software Engineering in Practice (IWESEP), 2012
Fourth International Workshop on},
title={QORAL: An External Domain-Specific Language for Mining Software
Repositories},
year={2012},
pages={23-29},
abstract={The mining software repositories (MSR) field integrates and
analyzes data stored in repositories such as source control and bug
repositories to provide support to practitioners. In order to provide
useful information to practitioners, MSR researchers need to perform
tasks iteratively, these tasks include extracting data from
repositories, transforming them into specific data formats, and loading
them into the statistical analysis tool. These tasks require a
significant amount of man hours to implement and execute according to
the requirements of the researchers. This paper proposes an external
domain-specific language (DSL) called QORAL to facilitate the
performance of multiple iterations and environment development. The
results from a questionnaire used to evaluate QORAL indicate that it is
easy to understand and modify source code.},
keywords={configuration management;data analysis;data mining;program
debugging;software engineering;DSL;MSR;QORAL;bug repository;bug tracking
system;data analysis;data extraction;data format;data
integration;environment development;external domain-specific
language;multiple iteration;software repository mining;source code
modification;source control;statistical analysis tool;version control
system;DSL;Data
mining;Grammar;Libraries;Loading;Measurement;Software;DSL;MSR;QORAL},
doi={10.1109/IWESEP.2012.20},
month={Oct},}
@ARTICLE{5487527,
author={T. Zimmermann and R. Premraj and N. Bettenburg and S. Just and
A. Schroter and C. Weiss},
journal={IEEE Transactions on Software Engineering},
title={What Makes a Good Bug Report?},
year={2010},
volume={36},
number={5},
pages={618-643},
abstract={In software development, bug reports provide crucial
information to developers. However, these reports widely differ in their
quality. We conducted a survey among developers and users of APACHE,
ECLIPSE, and MOZILLA to find out what makes a good bug report. The
analysis of the 466 responses revealed an information mismatch between
what developers need and what users supply. Most developers consider
steps to reproduce, stack traces, and test cases as helpful, which are,
at the same time, most difficult to provide for users. Such insight is
helpful for designing new bug tracking tools that guide users at
collecting and providing more helpful information. Our CUEZILLA
prototype is such a tool and measures the quality of new bug reports; it
also recommends which elements should be added to improve the quality.
We trained CUEZILLA on a sample of 289 bug reports, rated by developers
as part of the survey. The participants of our survey also provided 175
comments on hurdles in reporting and resolving bugs. Based on these
comments, we discuss several recommendations for better bug tracking
systems, which should focus on engaging bug reporters, better tool
support, and improved handling of bug duplicates.},
keywords={program debugging;program testing;software
quality;APACHE;CUEZILLA prototype;ECLIPSE;MOZILLA;bug tracking
tools;software development;Computer bugs;Debugging;Engineering
management;Human factors;Information
analysis;Programming;Prototypes;Software engineering;Software
maintenance;Software testing;Testing and debugging;and
enhancement;distribution;human
factors;maintenance;management;measurement.},
doi={10.1109/TSE.2010.63},
ISSN={0098-5589},
month={Sept},}
@INPROCEEDINGS{5631516,
author={F. Netto and M. O. Barros and A. C. F. Alvim},
booktitle={Software Engineering (SBES), 2010 Brazilian Symposium on},
title={An Automated Approach for Scheduling Bug Fix Tasks},
year={2010},
pages={80-89},
abstract={Even if a development team uses the best Software Engineering
practices to produce high-quality software, end users may find defects
that were not previously identified during the software development
life-cycle. These defects must be fixed and new versions of the software
incorporating the patches that solve them must be released. The project
manager must schedule a set of error correction tasks with different
priorities in order to minimize the time required to accomplish these
tasks and guarantee that the more important issues have been fixed.
Given the large number of distinct schedules, an automatically tool to
find good schedules may be helpful to project managers. This work
proposes a method which captures relevant information from bug
repositories and submits them to a genetic algorithm to find near
optimal bug correction task schedules. We have evaluated the approach
using a subset of the Eclipse bug repository and it suggested better
schedules than the actual schedules followed by Eclipse developers.},
keywords={genetic algorithms;program debugging;software quality;eclipse
bug repository;error correction tasks;genetic algorithm;high-quality
software;near optimal bug correction task schedules;software development
life-cycle;software engineering practices;Bismuth;Computer
bugs;Niobium;Programming;Schedules;Software;Software engineering;Search
based software engineering;bug resolution;task allocation},
doi={10.1109/SBES.2010.16},
month={Sept},}
@INPROCEEDINGS{4658104,
author={J. Anvik and M. A. Storey},
booktitle={Software Maintenance, 2008. ICSM 2008. IEEE International
Conference on},
title={Task articulation in software maintenance: Integrating source
code annotations with an issue tracking system},
year={2008},
pages={460-461},
abstract={Managing and articulating development tasks is an important
aspect of software maintenance. Developers already have a variety of
specialty tools to support task management, for example, issue tracking
and configuration management software, but they also make use of other
tools within their software engineering environments to support software
task management. An example of one such mechanism is the appropriation
of source code comments to document finer grained details of formally
specified tasks. In this research, we propose and present a tool that
integrates these source code annotations with an issue tracking
management system. We describe how this tool addresses deficiencies that
occur in task management and propose future research to improve task
management.},
keywords={groupware;software engineering;configuration management
software;issue tracking system;software engineering;software
maintenance;software task management;source code annotations;task
articulation;Collaborative software;Collaborative work;Computer
science;Engineering management;Environmental management;Software
development management;Software engineering;Software
maintenance;Software tools;Tagging},
doi={10.1109/ICSM.2008.4658104},
ISSN={1063-6773},
month={Sept},}
@INPROCEEDINGS{6360068,
author={T. Merten and T. Schäfer and S. Bürsner},
booktitle={Requirements Engineering Education and Training (REET), 2012
IEEE 7th International Workshop on},
title={Using RE knowledge to assist automatically during requirement
specification},
year={2012},
pages={9-13},
abstract={In a two semester software engineering (SE) course at
Bonn-Rhine-Sieg University students have the opportunity to actually
elicit, analyze and document requirements as well as design and develop
a correspondent software product in teams of approximately four. The
students have to use an issue tracking software in combination with a
Requirements Engineering (RE) tool to document and plan their work.
Though the course starts with RE theory from elicitation via
documentation and traceability, we found that the students find it
difficult to combine different RE artifact types and to develop useful
traces between them. In this paper we present an approach to provide
feedback and give pro-active advice inside an RE tool, while the
specification is created. To derive this feedback we use a knowledge
base containing rules and best practices to create a requirements
specification. An assistance system applies these rules to guide the
user in different situations, beginning with an empty specification up
to the implementation of various RE artifact types and traces between
them. This paper presents the status of our knowledge-based feedback
mechanism and possible extensions. In order to get primary indicators
for the value of this approach we did experiments and workshops with
eight students who worked with the same tool with and without the
feedback system.},
keywords={Cognition;Concrete;Documentation;Education;Knowledge based
systems;Knowledge engineering;Software;RE;direct feedback;knowledge
engineering;software-based feedback agents;teaching},
doi={10.1109/REET.2012.6360068},
month={Sept},}
@INPROCEEDINGS{6498458,
author={T. F. Bissyandé and F. Thung and S. Wang and D. Lo and L. Jiang
and L. Réveillère},
booktitle={Software Maintenance and Reengineering (CSMR), 2013 17th
European Conference on},
title={Empirical Evaluation of Bug Linking},
year={2013},
pages={89-98},
abstract={To collect software bugs found by users, development teams
often set up bug trackers using systems such as Bugzilla. Developers
would then fix some of the bugs and commit corresponding code changes
into version control systems such as svn or git. Unfortunately, the
links between bug reports and code changes are missing for many software
projects as the bug tracking and version control systems are often
maintained separately. Yet, linking bug reports to fix commits is
important as it could shed light into the nature of bug fixing processes
and expose patterns in software management. Bug linking solutions, such
as ReLink, have been proposed. The demonstration of their effectiveness
however faces a number of issues, including a reliability issue with
their ground truth datasets as well as the extent of their measurements.
We propose in this study a benchmark for evaluating bug linking
solutions. This benchmark includes a dataset of about 12,000 bug links
from 10 programs. These true links between bug reports and their fixes
have been provided during bug fixing processes. We designed a number of
research questions, to assess both quantitatively and qualitatively the
effectiveness of a bug linking tool. Finally, we apply this benchmark on
ReLink to report the strengths and limitations of this bug linking tool.},
keywords={program debugging;software engineering;Bugzilla system;ReLink
solution;bug linking empirical evaluation;bug linking tool;bug tracking
system;code change;ground truth dataset;software bug collection;software
management;software project;version control system;Benchmark
testing;Computer bugs;Control systems;Information retrieval;Joining
processes;Software;Training data;Bug Linking;ReLink;benchmark;empirical
evaluation;missing links},
doi={10.1109/CSMR.2013.19},
ISSN={1534-5351},
month={March},}
@INPROCEEDINGS{6976082,
author={R. K. Saha and J. Lawall and S. Khurshid and D. E. Perry},
booktitle={Software Maintenance and Evolution (ICSME), 2014 IEEE
International Conference on},
title={On the Effectiveness of Information Retrieval Based Bug
Localization for C Programs},
year={2014},
pages={161-170},
abstract={Localizing bugs is important, difficult, and expensive,
especially for large software projects. To address this problem,
information retrieval (IR) based bug localization has increasingly been
used to suggest potential buggy files given a bug report. To date,
researchers have proposed a number of IR techniques for bug localization
and empirically evaluated them to understand their effectiveness.
However, virtually all of the evaluations have been limited to the
projects written in object-oriented programming languages, particularly
Java. Therefore, the effectiveness of these techniques for other widely
used languages such as C is still unknown. In this paper, we create a
benchmark dataset consisting of more than 7,500 bug reports from five
popular C projects and rigorously evaluate our recently introduced
IR-based bug localization tool using this dataset. Our results indicate
that although the IR-relevant properties of C and Java programs are
different, IR-based bug localization in C software at the file level is
overall as effective as in Java software. However, we also find that the
recent advance of using program structure information in performing bug
localization gives less of a benefit for C software than for Java
software.},
keywords={C language;Java;information retrieval;object-oriented
programming;program debugging;C programs;Java programs;bug
localization;buggy files;information retrieval;object-oriented
programming languages;software
projects;Accuracy;Java;Kernel;Linux;Measurement;Bug
Localization;Information Retrieval;Search},
doi={10.1109/ICSME.2014.38},
ISSN={1063-6773},
month={Sept},}
@INPROCEEDINGS{5431740,
author={Y. Ki and M. Song},
booktitle={Automated Software Engineering, 2009. ASE '09. 24th IEEE/ACM
International Conference on},
title={An Open Source-Based Approach to Software Development
Infrastructures},
year={2009},
pages={525-529},
abstract={As software systems become larger and more complex, automated
software engineering tools play a crucial role for effective software
development management, which is a key factor to lead quality software
systems. In this work, we present TRICA, an open source-based software
development infrastructure. The name of TRICA represents its features
such as Traceability, Relationship, Informativeness, Cost-effectiveness,
and Automation. Essentially, in TRICA, a continuous integration tool is
coupled with a software configuration management tool and an issue
tracking tool. We provisioned a mechanism to connect the open source
tools in TRICA so that project members use the collaborated information
to solve various issues and implementation problems efficiently, and
easily share forthcoming issues during the course of the project. We
show that TRICA can help to decentralize risks throughout the software
development cycle and achieve successful software development.},
keywords={configuration management;groupware;integrated software;program
diagnostics;public domain software;software development
management;software quality;software tools;TRICA;automated software
engineering tools;collaborated information;continuous integration
tool;open source tools;software configuration management tool;software
development cycle;software development management;software
quality;software systems;tracking tool;Automation;Collaborative
software;Collaborative tools;Open source software;Programming;Software
development management;Software engineering;Software systems;Software
tools;Testing;SCM;continuous integration;issue tracking;open
source;software engineering tools},
doi={10.1109/ASE.2009.73},
ISSN={1938-4300},
month={Nov},}
@ARTICLE{7270328,
author={X. Ye and R. Bunescu and C. Liu},
journal={IEEE Transactions on Software Engineering},
title={Mapping Bug Reports to Relevant Files: A Ranking Model, a
Fine-grained Benchmark, and Feature Evaluation},
year={2015},
volume={PP},
number={99},
pages={1-1},
abstract={When a new bug report is received, developers usually need to
reproduce the bug and perform code reviews to find the cause, a process
that can be tedious and time consuming. A tool for ranking all the
source files with respect to how likely they are to contain the cause of
the bug would enable developers to narrow down their search and improve
productivity. This paper introduces an adaptive ranking approach that
leverages project knowledge through functional decomposition of source
code, API descriptions of library components, the bug-fixing history,
the code change history, and the file dependency graph. Given a bug
report, the ranking score of each source file is computed as a weighted
combination of an array of features, where the weights are trained
automatically on previously solved bug reports using a learning-to-rank
technique. We evaluate the ranking system on six large scale open source
Java projects, using the before-fix version of the project for every bug
report. The experimental results show that the learning-to-rank approach
outperforms three recent state-of-the-art methods. In particular, our
method makes correct recommendations within the top 10 ranked source
files for over 70% of the bug reports in the Eclipse Platform and Tomcat
projects.},
keywords={Benchmark testing;Collaboration;Computational
modeling;Computer bugs;History;Software;Standards;Bug reports;learning
to rank;software maintenance},
doi={10.1109/TSE.2015.2479232},
ISSN={0098-5589},
month={},}
@ARTICLE{722227,
author={J. R. Callahan and R. R. Khatsuriya and R. Hefner},
journal={IEEE Internet Computing},
title={Web-based issue tracking for large software projects},
year={1998},
volume={2},
number={5},
pages={25-33},
abstract={Many problems are found and fixed during the development of a
software system. The Project Issue Tracking System toolkit, a Web-based
issue-management tool, can be used to organize issue reports during
development and to communicate with different project teams around the
world. The Project Issue Tracking System (PITS) is a tool that supports
the IV&V effort for two major NASA projects: the Earth Observation
System Data and Information System (EOSDIS) and the Earth Science Data
and Information System (ESDIS). The EOSDIS IV&V effort teams several
companies and organizations at several sites with Intermetrics, Inc,
serving as the lead contractor. We examine the PITS Web-based mechanisms
for tracking issue reports},
keywords={Internet;computer aided software engineering;geophysics
computing;scientific information systems;software development
management;Earth Observation System Data and Information System;Earth
Science Data and Information System;Intermetrics;Internet;NASA
projects;PITS;Project Issue Tracking System;World Wide Web;issue
tracking;issue-management tool;large software projects;software
development;Communication effectiveness;Computer
errors;Databases;Environmental management;Geoscience;Information
systems;NASA;Personnel;Software development management;Software tools},
doi={10.1109/4236.722227},
ISSN={1089-7801},
month={Sep},}
@INPROCEEDINGS{6224296,
author={I. Keivanloo and C. Forbes and A. Hmood and M. Erfani and C.
Neal and G. Peristerakis and J. Rilling},
booktitle={Mining Software Repositories (MSR), 2012 9th IEEE Working
Conference on},
title={A Linked Data platform for mining software repositories},
year={2012},
pages={32-35},
abstract={The mining of software repositories involves the extraction of
both basic and value-added information from existing software
repositories. The repositories will be mined to extract facts by
different stakeholders (e.g. researchers, managers) and for various
purposes. To avoid unnecessary pre-processing and analysis steps,
sharing and integration of both basic and value-added facts are needed.
In this research, we introduce SeCold, an open and collaborative
platform for sharing software datasets. SeCold provides the first online
software ecosystem Linked Data platform that supports data extraction
and on-the-fly inter-dataset integration from major version control,
issue tracking, and quality evaluation systems. In its first release,
the dataset contains about two billion facts, such as source code
statements, software licenses, and code clones from 18 000 software
projects. In its second release the SeCold project will contain
additional facts mined from issue trackers and versioning systems. Our
approach is based on the same fundamental principle as Wikipedia:
researchers and tool developers share analysis results obtained from
their tools by publishing them as part of the SeCold portal and
therefore make them an integrated part of the global knowledge domain.
The SeCold project is an official member of the Linked Data dataset
cloud and is currently the eighth largest online dataset available on
the Web.},
keywords={data mining;software packages;code clones;collaborative
platform;linked data platform;mining software repositories;on-the-fly
inter-dataset integration;online software ecosystem linked data
platform;software datasets;software licenses;software packages;software
repositories;source code statements;value added
information;Cloning;Communities;Data
mining;Encyclopedias;Licenses;Ontologies;Software;Linked Data;fact
sharing;software mining},
doi={10.1109/MSR.2012.6224296},
ISSN={2160-1852},
month={June},}
@INPROCEEDINGS{5196955,
author={M. Legenhausen and S. Pielicke and J. Rühmkorf and H. Wendel and
A. Schreiber},
booktitle={Global Software Engineering, 2009. ICGSE 2009. Fourth IEEE
International Conference on},
title={RepoGuard: A Framework for Integration of Development Tools with
Source Code Repositories},
year={2009},
pages={328-331},
abstract={Today modern software development is not possible without the
aid of tools like version control systems, bug tracking systems or
instruments that ensure the compliance with code conventions.
Unfortunately, all of these tools ldquolive in their own worldrdquo, are
only loosely coupled and do not interact with each other. RepoGuard
addresses this problem by linking version control systems to other
software development tools. It is implemented as an extension to several
version control systems and provides interfaces to integrate other
tools. The use of RepoGuard allows maximum control and validation of all
committed resources before they are permanently stored. Additionally,
RepoGuard provides communication channels in order to inform all
relevant stakeholders about the failure or success of the process.
Overall, RepoGuard provides simple but effective means to guarantee
software quality standards in distributed development processes.},
keywords={distributed processing;software quality;software tools;source
coding;RepoGuard;development tools;distributed development
process;software development;software quality;source code
repositories;Communication system control;Computer languages;Control
systems;Engines;Java;Performance
evaluation;Permission;Programming;Software quality;Testing},
doi={10.1109/ICGSE.2009.51},
month={July},}
@INPROCEEDINGS{7081860,
author={J. L. C. Izquierdo and V. Cosentino and B. Rolandi and A. Bergel
and J. Cabot},
booktitle={Software Analysis, Evolution and Reengineering (SANER), 2015
IEEE 22nd International Conference on},
title={GiLA: GitHub label analyzer},
year={2015},
pages={479-483},
abstract={Reporting bugs, asking for new features and in general giving
any kind of feedback is a common way to contribute to an Open-Source
Software (OSS) project. In GitHub, the largest code hosting service for
OSS, this feedback is typically expressed as new issues for the project
managed by an issue-tracking system available in each new project
repository. Among other features, the issue tracker allows creating and
assigning labels to issues with the goal of helping the project
community to better classify and manage those issues (e.g., facilitating
the identification of issues for top priority components or candidate
developers that could solve them). Nevertheless, as the project grows a
manual browsing of the project issues is no longer feasible. In this
paper we present GiLA, a tool which generates a set of visualizations to
facilitate the analysis of issues in a project depending on their
label-based categorization. We believe our visualizations are useful to
see the most popular labels (and their relationships) in a project,
identify the most active community members for those labels and compare
the typical issue evolution for each label category.},
keywords={program visualisation;project management;public domain
software;software development management;source code
(software);GiLA;GitHub label analyzer;OSS project;active community
members;code hosting service;issue tracker;issue-tracking
system;label-based categorization;open-source software project;project
community;project management;project
repository;visualizations;Communities;Computer
architecture;Databases;Servers;Visualization;Web services},
doi={10.1109/SANER.2015.7081860},
month={March},}
@INPROCEEDINGS{6798341,
author={D. Behl and S. Handa and A. Arora},
booktitle={Optimization, Reliabilty, and Information Technology
(ICROIT), 2014 International Conference on},
title={A bug Mining tool to identify and analyze security bugs using
Naive Bayes and TF-IDF},
year={2014},
pages={294-299},
abstract={Bug report contains a vital role during software development,
However bug reports belongs to different categories such as performance,
usability, security etc. This paper focuses on security bug and presents
a bug mining system for the identification of security and non-security
bugs using the term frequency-inverse document frequency (TF-IDF)
weights and naïve bayes. We performed experiments on bug report
repositories of bug tracking systems such as bugzilla and debugger. In
the proposed approach we apply text mining methodology and TF-IDF on the
existing historic bug report database based on the bug s description to
predict the nature of the bug and to train a statistical model for
manually mislabeled bug reports present in the database. The tool helps
in deciding the priorities of the incoming bugs depending on the
category of the bugs i.e. whether it is a security bug report or a
non-security bug report, using naïve bayes. Our evaluation shows that
our tool using TF-IDF is giving better results than the naïve bayes
method.},
keywords={Bayes methods;data mining;security of data;statistical
analysis;text analysis;Naive Bayes method;TF-IDF;bug mining tool;bug
tracking systems;historic bug report database;nonsecurity bug
identification;nonsecurity bug report;security bug report;security bugs
identification;software development;statistical model;term
frequency-inverse document frequency weights;text mining
methodology;Computer bugs;Integrated circuit modeling;Vectors;Bug;Naïve
Bayes;TF-IDF;mining;non-security bug report;security bug reports;text
analysis},
doi={10.1109/ICROIT.2014.6798341},
month={Feb},}
@INPROCEEDINGS{7087237,
author={D. Hansson},
booktitle={Microprocessor Test and Verification Workshop (MTV), 2014
15th International},
title={Continuous Linting with Automatic Debug},
year={2014},
pages={70-72},
abstract={Lint tools analyze RTL statically and report code segments
that do not comply with the selected coding guidelines. It is quick to
run and as the error messages are very precise it is easy to fix the
issues. It requires much less resources to fix a linting issue than to
find and fix the same issue during simulation of a test. However, the
large amounts of error and warning messages that the linting tools
produce is a problem. Developers tend to push out linting until the very
last moment just before the release. The engineers have plenty of
critical test failures to attend to first, so it is a rational decision.
However, this does not use the full potential of linting, because
linting errors are very quick to fix, and some of the reported linting
issues will be the same issues that takes much longer to debug during
simulation. Pushing linting to the end of the project misses the
opportunity to save a lot of time, but on the other hand developers
cannot waste time by running the lint tool every now and again
just-in-case they would find some good bugs. The solution is what we
call continuous linting, where we combine the linting tool with an
automatic debug tool of regression failures. At the start of the project
we select the subset of the coding guidelines that the design must
comply to. The automatic debug tool then runs the linting tool
regularly, e.g. Each night, and when a linting issue is detected then
the automatic debug tool sends a bug report to each individual with a
list of linting errors this person has caused and in which commit they
were introduced. We believe this unleashes the full potential of
linting. By reporting the issues immediately to the individual that
caused them instead of waiting to the end of the project, issues are
fixed earlier at no extra human effort. The issues have to be fixed at
some point and better sooner than later. This is all made possible by
the automatic debug tool. As the engineers will fix their linting issues
immediately- this will probably save time as some of the simulation bugs
will be fixed faster and with less effort as more issues will be fixed
due to linting error messages and fewer by manually analyzing simulation
failures. In this paper we present the experience from a real ASIC
project using continuous linting. We measured the number of linting
issues that the automatic debug tool reported over a two month period.
At the end of this period there were 0 linting issues outstanding. Had
we waited until the end of the project to look at the lint issues there
would have been 3617 lint issues in total to analyze. This is bound to
have a significant positive effect on the project. The cost of fixing
linting issues immediately by the person that just caused them is much
less than for an engineer to sift through large amounts of lint errors
and warnings at the end of the project.},
keywords={application specific integrated circuits;computer
debugging;program debugging;program testing;software fault
tolerance;software tools;source code (software);ASIC
project;RTL;automatic debug tool;code segments;coding
guidelines;continuous linting;lint warnings;linting error
messages;linting issues;linting tools;regression failures;regression
testing;simulation bugs;simulation failures;Application specific
integrated circuits;Computer bugs;Encoding;Guidelines;IEEE
Potentials;Manuals;Testing;automatic debug;continuous linting;regression
testing},
doi={10.1109/MTV.2014.25},
ISSN={1550-4093},
month={Dec},}
@INPROCEEDINGS{7070480,
author={L. An and F. Khomh},
booktitle={Software Analytics (SWAN), 2015 IEEE 1st International
Workshop on},
title={Challenges and Issues of Mining Crash Reports},
year={2015},
pages={5-8},
abstract={Automatic crash reporting tools built in many software systems
allow software practitioners to understand the origin of field crashes
and help them prioritise field crashes or bugs, locate erroneous files,
and/or predict bugs and crash occurrences in subsequent versions of the
software systems. In this paper, after illustrating the structure of
crash reports in Mozilla, we discuss some techniques for mining
information from crash reports, and highlight the challenges and issues
of these techniques. Our aim is to raise the awareness of the research
community about issues that may bias research results obtained from
crash reports and provide some guidelines to address certain challenges
related to mining crash reports.},
keywords={data mining;program debugging;program testing;crash report
mining;crash reporting tool;software systems;Algorithm design and
analysis;Computer bugs;Data mining;Databases;Software systems;Crash
report;bug report;mining softwarerepositories.},
doi={10.1109/SWAN.2015.7070480},
month={March},}
@INPROCEEDINGS{5676263,
author={D. Wang and M. Lin and H. Zhang and H. Hu},
booktitle={Computer Software and Applications Conference (COMPSAC), 2010
IEEE 34th Annual},
title={Detect Related Bugs from Source Code Using Bug Information},
year={2010},
pages={228-237},
abstract={Open source projects often maintain open bug repositories
during development and maintenance, and the reporters often point out
straightly or implicitly the reasons why bugs occur when they submit
them. The comments about a bug are very valuable for developers to
locate and fix the bug. Meanwhile, it is very common in large software
for programmers to override or overload some methods according to the
same logic. If one method causes a bug, it is obvious that other
overridden or overloaded methods maybe cause related or similar bugs. In
this paper, we propose and implement a tool Rebug-Detector, which
detects related bugs using bug information and code features. Firstly,
it extracts bug features from bug information in bug repositories;
secondly, it locates bug methods from source code, and then extracts
code features of bug methods; thirdly, it calculates similarities
between each overridden or overloaded method and bug methods; lastly, it
determines which method maybe causes potential related or similar bugs.
We evaluate Rebug-Detector on an open source project: Apache
Lucene-Java. Our tool totally detects 61 related bugs, including 21 real
bugs and 10 suspected bugs, and it costs us about 15.5 minutes. The
results show that bug features and code features extracted by our tool
are useful to find real bugs in existing projects.},
keywords={Java;program debugging;public domain software;source
coding;Apache Lucene-Java;Rebug- Detector;Rebug-Detector;bug
detection;bug features;bug information;bug repository;code features;open
source project;source code;Computer bugs;Data
mining;Documentation;Feature extraction;Java;Programming;Software;bug
detection;bug features;bug information;code features;common substring},
doi={10.1109/COMPSAC.2010.27},
ISSN={0730-3157},
month={July},}
@INPROCEEDINGS{7039122,
author={G. Mauša and T. G. Grbac and B. D. Bašić},
booktitle={Software, Telecommunications and Computer Networks (SoftCOM),
2014 22nd International Conference on},
title={Software defect prediction with Bug-Code analyzer - A data
collection tool demo},
year={2014},
pages={425-426},
abstract={Empirical software engineering research community aims to
accumulate knowledge in software engineering community based on the
empirical studies on datasets obtained from the real software projects.
Limiting factor to building the theory over thus accumulated knowledge
is often related to dataset bias. One solution to this problem is
developing a systematic data collection procedure through standard
guidelines that would be available to open community and thus enable
reducing data collection bias. In this paper we present a tool
demonstration that implements a systematic data collection procedure for
software defect prediction datasets from the open source bug tracking
and the source code management repositories. Main challenging issue that
the tool addresses is linking the information related to the same entity
(e.g. class file) from these two sources. The tool implements interfaces
to bug and source code repositories and even other tools for calculating
the software metrics. Finally, it offers the user to create software
defect prediction datasets even if he is unaware of all the details
behind this complex task.},
keywords={program debugging;program testing;public domain
software;software metrics;source code (software);bug-code analyzer;class
file;complex task;data collection bias reduction;data collection
tool;empirical software engineering research community;open
community;open source bug tracking;real software projects;software
defect prediction datasets;software metrics;source code management
repositories;standard guidelines;systematic data collection
procedure;Communities;Computer bugs;Data collection;Joining
processes;Measurement;Software;Software engineering;Automated
Tool;Mining Software Repositories;Software Defect Prediction},
doi={10.1109/SOFTCOM.2014.7039122},
month={Sept},}
@INPROCEEDINGS{6747208,
author={T. Dal Sasso and M. Lanza},
booktitle={Software Maintenance, Reengineering and Reverse Engineering
(CSMR-WCRE), 2014 Software Evolution Week - IEEE Conference on},
title={In #x2217;bug: Visual analytics of bug repositories},
year={2014},
pages={415-419},
abstract={Bug tracking systems are used to track and store the defects
reported during the life of software projects. The underlying
repositories represent a valuable source of information used for example
for defect prediction and program comprehension. However, bug tracking
systems present the actual bugs essentially in textual form, which is
not only cumbersome to navigate, but also hinders the understanding of
the intricate pieces of information that revolve around software bugs.
We present in*Bug, a web-based visual analytics platform to navigate and
inspect bug repositories. in*Bug provides several interactive views to
understand detailed information about the bugs and the people that
report them. The tool can be downloaded at http://inbug.inf.usi.ch},
keywords={data visualisation;information storage;program
debugging;software tools;Web-based visual analytics platform;bug
repositories;bug tracking systems;defect prediction;in*Bug;information
repositories;program comprehension;software bugs;software
projects;Complexity theory;Computer bugs;Data
visualization;Navigation;Software;Visual analytics},
doi={10.1109/CSMR-WCRE.2014.6747208},
month={Feb},}
@INPROCEEDINGS{6462726,
author={X. Wu and J. Wei and X. Wang},
booktitle={Software Engineering Conference (APSEC), 2012 19th
Asia-Pacific},
title={Debug Concurrent Programs with Visualization and Inference of
Event Structure},
year={2012},
volume={1},
pages={683-692},
abstract={Owing to the inherent non-determinism of concurrent programs,
traditional debugging techniques are not sufficient, especially in the
scene of postmortem debugging. Exploring defects through static analysis
can avoid the high cost of failure reproduction. In this paper, we
present a novel debugging method for concurrency bugs. We make use of
information in bug reports and slicing techniques, construct an event
structure model from Java program to reveal the program behaviors
related to the suspicious variables and methods identified from bug
reports. Utilize the nature of event structure, we extract a small but
sufficient subset from all possible execution traces and visualize them
with a graph. This method can effectively help to comprehend concurrent
system and assist in locating concurrency bugs. We have implemented a
tool called JESVisT (Java event structure visualization Tool) to support
this method based on open-source projects Bandera and Indus.},
keywords={Java;concurrency control;program debugging;program
diagnostics;program visualisation;public domain
software;Bandera;Indus;JESVisT tool;Java event structure visualization
tool;Java program;bug report;concurrency bug;concurrent
program;concurrent system;debugging method;debugging technique;event
structure model;failure reproduction cost;graph
visualization;open-source project;postmortem debugging;program
behavior;slicing technique;static analysis;Computer bugs;Concurrent
computing;Debugging;Java;Software;Testing;Visualization;Debug;concurrent
programs;event structure;visualization},
doi={10.1109/APSEC.2012.134},
ISSN={1530-1362},
month={Dec},}
@INPROCEEDINGS{6926095,
author={D. Hansson and H. Uronen-Hansson},
booktitle={Microprocessor Test and Verification (MTV), 2013 14th
International Workshop on},
title={Measuring the Gain of Automatic Debug},
year={2013},
pages={19-22},
abstract={The purpose of regression testing is to quickly catch any
deterioration in quality of a product under development. The more
frequently tests are run, the earlier new issues can be detected
resulting in a larger burden for the engineers who need to manually
debug all test failures, many of which are failing due to the same
underlying bug. However, there are software tools that automatically
debug the test failures back to the faulty change and notifies the
engineer who made this change. By analyzing data from a real commercial
ASIC project we aimed to measure whether bugs are fixed faster when
using automatic debug tools compared to manual debugging. All bugs in an
ASIC development project were analyzed over a period of 3 months in
order to determine the time it took the bug to be fixed and to compare
the results from both automatic and manual debug. By measuring the time
from when the bug report was sent out by the automatic debug tool until
the bug was fixed, we can show that bugs are fixed 4 times faster with
automatic debug enabled. Bug fixing time was on average 5.7 hours with
automatic debug and 23.0 hours for manual debug. The result was achieved
by comparing bugs that were automatically debugged to those issues that
could not be debugged by the tool, because those issues were outside the
defined scope of the device under test. Such issues are still reported
by the automatic debug tool but marked as requiring manual debug and is
consequently a good point of comparison. A 4 times quicker bug fixing
process is significant and can ultimately contribute to a shortening of
a development project as the bug turnaround time is one of the key
aspects defining the length of a project, especially in the later phase
just before release.},
keywords={application specific integrated circuits;integrated circuit
testing;program debugging;regression analysis;ASIC development
project;automatic debug tools;bug fixing process;bug report;bug
turnaround time;commercial ASIC project;manual debugging;regression
testing;software tools;test failures debug;Application specific
integrated circuits;Computer bugs;Delays;Electronic
mail;Manuals;Testing;automatic debug;continuous integration;regression
testing;system-on-chip verification},
doi={10.1109/MTV.2013.17},
ISSN={1550-4093},
month={Dec},}
@ARTICLE{1407819,
author={N. Serrano and I. Ciordia},
journal={IEEE Software},
title={Bugzilla, ITracker, and other bug trackers},
year={2005},
volume={22},
number={2},
pages={11-13},
abstract={Bug-tracking helps the software developers in knowing what the
error is, resolving it, and learning from it. Working on a software
project includes managing the bugs we find. At first, we might list them
on a spreadsheet. But when the number of bugs becomes too large and a
lot of people must access and input data on them, we have to give up the
spreadsheet and instead use a bug- or issue-tracking system. Many
software projects reach this point, especially during testing and
deployment when users tend to find an application's bugs. Nowadays we
can choose among dozens of bug-tracking systems. This paper looks at two
specific open source products and provides useful hints for working with
any bug-tracking tool.},
keywords={program debugging;program diagnostics;program testing;public
domain software;software tools;bug-tracking system;bug-tracking
tool;open source product;program debugging;program diagnostics;program
testing;Application software;Computer bugs;Control
systems;Filters;Information systems;Linux;NASA;Open source
software;Software tools;Spatial databases;bug database;bug
tracker;issues database},
doi={10.1109/MS.2005.32},
ISSN={0740-7459},
month={March},}
@INPROCEEDINGS{1698774,
author={A. J. Ko and B. A. Myers and Duen Horng Chau},
booktitle={Visual Languages and Human-Centric Computing, 2006. VL/HCC
2006. IEEE Symposium on},
title={A Linguistic Analysis of How People Describe Software Problems},
year={2006},
pages={127-134},
abstract={There is little understanding of how people describe software
problems, but a variety of tools solicit, manage, and analyze these
descriptions in order to streamline software development. To inform the
design of these tools and generate ideas for new ones, an study of
nearly 200,000 bug report titles was performed. The titles of the
reports generally described a software entity or behavior, its
inadequacy, and an execution context, suggesting new designs for more
structured report forms. About 95% of noun phrases referred to visible
software entities, physical devices, or user actions, suggesting the
feasibility of allowing users to select these entities in debuggers and
other tools. Also, the structure of the titles exhibited sufficient
regularity to parse with an accuracy of 89%, enabling a number of new
automated analyses. These findings and others have many implications for
tool design and software engineering},
keywords={computational linguistics;software engineering;linguistic
analysis;software development;software engineering;software
problems;tool design;Open source software;Programming;Software
debugging;Software design;Software development management;Software
engineering;Software maintenance;Software tools;Usability;User interfaces},
doi={10.1109/VLHCC.2006.3},
month={Sept},}
@INPROCEEDINGS{5783175,
author={E. Stepalina},
booktitle={Software Engineering Conference (CEE-SECR), 2010 6th Central
and Eastern European},
title={SaaS support in software documentation systems},
year={2010},
pages={192-197},
abstract={In recent days more and more software developments tools
become distributed by the SaaS (Software-As-A Service) model alongside
with ready-to-install products. The developers of task and bug tracking
systems now offer their solutions by a monthly fee. For instance, JIRA
Studio produced by Atlassian can be connected to a corporative domain by
subscription. This scheme allows software companies to reduce costs at
the project's start and get scalable resources in future. Software
documentation systems can also be purchased by a subscription now. The
effectiveness of their usage for various documentation development is
interesting. There are four major types of documentation supporting the
development process and resulted products: project, technical, code and
user documentation. Each of this type claims specific requirements for
the documentation tool. The requirement analysis shows that rented
documentation systems are the most appropriate for user and technical
documentation. There are two major classes of software documentation
systems: 1) Wiki, 2) DITA-orientedXML CMS. The following wiki systems
have a hosted version: commercial Confluence, Central Desktop, EditMe,
Incentive, Netcipia, PBWiki, Wikia, Wikispaces; open source
BusinessWiki, Metadot Wiki, MindTouch, Wagn, Wikidot. The richest by the
functionality andplugin collection is Confluence produced by Atlassian.
The following XML CMS are offered by a SaaS model (all are commercial):
Astoria On Demand, DITA Exchange. DocZone. SaaS is optionally supported
in Bluestream XDocs, Siberlogic SiberSafe, Trisoft Infoshare, Vasont,
X-Hive Docato. As wiki system is a ready integrated environment for
creating and publishing documentation, DITA-system consists not only of
XML CMS. To deploy a DITA-system, you should have an XML editor,
publisher and CMS. The listed CMS can be integrated with top DITA XML
editors and provide an API to integrate with other editors. These CMS
also have build-in tools to export documents in mu- - ltiple formats.
However, the universal component architecture of DITA-systems makes the
deployment and configuration more difficult than wiki implementation.
Hosted documentation systems are offered by different prices. The
offerings of top documentation systems are considered in this paper.
Wiki subscription fees range from 4,95$ (EditMe) to 20$ (Confluence) per
one user/month. XML CMS subscription price starts from 500$ per month
and can reach 12000$ per month. These subscriptions have no fixed price;
in each individual case the CMS vendor performs a specific project of a
DITA-system implementation. Wiki rental costs approximate to CMS
subscriptions' costs for large number of users, 500 and more. The
advantages of renting a powerful documentation system for small and
large project are the following: 1) Maximal functionality at a low
affordable cost, 2) Platform independency and high system accessibility,
3) Document quality improvement at the expense of quality controlling
tools application, 4) Higher effectiveness of documentation (content
re-use, single source usage, automated tools for localization), 5)
Organization of robust and scalable documentation process. As the SaaS
business model becomes more popular, small companies get access to
powerful software documentation systems, which are too expensive to
purchase a standalone license at the startup. However, the system's
access security, reliability and information confidentiality issues
remain opened and controversial.},
keywords={Web sites;XML;cloud computing;document handling;software
engineering;systems analysis;τasont;Astoria On
Demand;Atlassian;Bluestream XDocs;Central Desktop;Confluence;DITA
Exchange;DITA-orientedXML CMS;DocZone;EditMe;Incentive;JIRA
Studio;Metadot Wiki;MindTouch;Netcipia;PBWiki;SaaS business model;SaaS
support;Siberlogic SiberSafe;Trisoft
Infoshare;Wagn;Wikia;Wikidot;Wikispaces;X-Hive Docato;bug tracking
systems;documentation publishing;open source BusinessWiki;quality
controlling tools application;ready-to-install products;requirement
analysis;software companies;software developments tools;software
documentation systems;task tracking systems;Documentation;Electronic
publishing;Google;Information
services;Internet;Subscriptions;XML;SaaS;Software documentation;XML
CMS;wiki},
doi={10.1109/CEE-SECR.2010.5783175},
month={Oct},}
@INPROCEEDINGS{6345811,
author={E. Knauss and D. Damian and G. Poo-Caamaño and J. Cleland-Huang},
booktitle={Requirements Engineering Conference (RE), 2012 20th IEEE
International},
title={Detecting and classifying patterns of requirements clarifications},
year={2012},
pages={251-260},
abstract={In current project environments, requirements often evolve
throughout the project and are worked on by stakeholders in large and
distributed teams. Such teams often use online tools such as mailing
lists, bug tracking systems or online discussion forums to communicate,
clarify or coordinate work on requirements. In this kind of environment,
the expected evolution from initial idea, through clarification, to a
stable requirement, often stagnates. When project managers are not aware
of underlying problems, development may proceed before requirements are
fully understood and stabilized, leading to numerous implementation
issues and often resulting in the need for early redesign and
modification. In this paper, we present an approach to analyzing online
requirements communication and a method for the detection and
classification of clarification events in requirement discussions. We
used our approach to analyze online requirements communication in the
IBM^® Rational Team Concert^® (RTC) project and identified a set of six
clarification patterns. Since a predominant amount of clarifications
through the lifetime of a requirement is often indicative of problematic
requirements, our approach lends support to project managers to assess,
in real-time, the state of discussions around a requirement and promptly
react to requirements problems.},
keywords={distributed processing;formal verification;pattern
classification;project management;systems analysis;IBM^® Rational Team
Concert^® project;RTC project;clarification event
classification;clarification event detection;distributed teams;online
requirement communication;project environments;project
managers;requirement clarification pattern;Context;Manuals;Message
systems;Natural
languages;Software;Trajectory;Visualization;communication of
requirements;distributed requirements engineering;requirements
clarification patterns},
doi={10.1109/RE.2012.6345811},
ISSN={1090-750X},
month={Sept},}
@INPROCEEDINGS{5283228,
author={C. A. Ardagna and M. Banzi and E. Damiani and F. Frati},
booktitle={Computational Science and Engineering, 2009. CSE '09.
International Conference on},
title={Assurance Process for Large Open Source Code Bases},
year={2009},
volume={3},
pages={412-417},
abstract={Many organizations are investigating the possibility of
adopting open source software or migrating their mission critical
applications to open platforms. In this context, defining an assurance
process for large open source code bases has becomes of paramount
importance, and can help in filling the gap with proprietary solutions.
In this paper, we discuss how assurance has become a primary requirement
for organizations wishing to adopt open source products. Then, we
describe how bug tracking and fixing can be enriched to support a more
general cycle of assurance, and we evaluate how this process can be
applied to large-scale open source projects like JADE and WADE.},
keywords={program debugging;public domain software;software
engineering;JADE;WADE;bug fixing;bug tracking;large open source code
bases;open source software;software assurance;Computer
architecture;Kernel;Linux;Mission critical systems;Open source
software;Programming;Security;Software quality;Technological
innovation;Telecommunication computing;Issue Tracking;JADE;Opne
Source;Software Assurance;WADE},
doi={10.1109/CSE.2009.271},
month={Aug},}
@INPROCEEDINGS{7181431,
author={T. D. B. Le and M. Linares-Vasquez and D. Lo and D. Poshyvanyk},
booktitle={Program Comprehension (ICPC), 2015 IEEE 23rd International
Conference on},
title={RCLinker: Automated Linking of Issue Reports and Commits
Leveraging Rich Contextual Information},
year={2015},
pages={36-47},
abstract={Links between issue reports and their corresponding commits in
version control systems are often missing. However, these links are
important for measuring the quality of various parts of a software
system, predicting defects, and many other tasks. A number of existing
approaches have been designed to solve this problem by automatically
linking bug reports to source code commits via comparison of textual
information in commit messages with textual contents in the bug reports.
Yet, the effectiveness of these techniques is oftentimes sub optimal
when commit messages are empty or only contain minimum information, this
particular problem makes the process of recovering trace ability links
between commits and bug reports particularly challenging. In this work,
we aim at improving the effectiveness of existing bug linking techniques
by utilizing rich contextual information. We rely on a recently proposed
tool, namely Change Scribe, which generates commit messages containing
rich contextual information by using a number of code summarization
techniques. Our approach then extracts features from these automatically
generated commit messages and bug reports and inputs them into a
classification technique that creates a discriminative model used to
predict if a link exists between a commit message and a bug report. We
compared our approach, coined as RCLinker (Rich Context Linker), to
MLink, which is an existing state-of-the-art bug linking approach. Our
experiment results on bug reports from 6 software projects show that
RCLinker can outperform MLink in terms of F-measure by 138.66%.},
keywords={configuration management;program debugging;program
diagnostics;software quality;source code
(software);ChangeScribe;F-measure;MLink;RCLinker;Rich-Context
Linker;automated bug issue report linking;automated source code commit
linking;classification technique;code summarization techniques;commit
messages;defect prediction;discriminative model;rich-contextual
information leveraging;software projects;software system quality;textual
information;traceability links;version control systems;Control
systems;Feature extraction;Joining processes;Metadata;Predictive
models;Training;XML;ChangeScribe;Classification;Feature
Extraction;Recovering Missing Links},
doi={10.1109/ICPC.2015.13},
month={May},}
@INPROCEEDINGS{6062192,
author={Y. Song and X. Wang and T. Xie and L. Zhang and H. Mei},
booktitle={Software Engineering, 2010 ACM/IEEE 32nd International
Conference on},
title={JDF: detecting duplicate bug reports in Jazz},
year={2010},
volume={2},
pages={315-316},
abstract={Both developers and users submit bug reports to a bug
repository. These reports can help reveal defects and improve software
quality. As the number of bug reports in a bug repository increases, the
number of the potential duplicate bug reports increases. Detecting
duplicate bug reports helps reduce development efforts in fixing
defects. However, it is challenging to manually detect all potential
duplicates because of the large number of existing bug reports. This
paper presents JDF (representing Jazz Duplicate Finder), a tool that
helps users to find potential duplicates of bug reports on Jazz, which
is a team collaboration platform for software development and process
management. JDF finds potential duplicates for a given bug report using
natural language and execution information.},
keywords={groupware;natural language processing;software development
management;software quality;team working;JDF;bug reports;bug
repository;execution information;natural language;process
management;software development;software quality;team
collaboration;Calculators;Computer architecture;Educational
institutions;Measurement;Natural languages;Servers;Software;bug
report;execution information;information retrieval},
doi={10.1145/1810295.1810368},
ISSN={0270-5257},
month={May},}
