% This file was created with JabRef 2.10.
% Encoding: UTF-8


@InProceedings{20142017728931,
  Title                    = {A bug Mining tool to identify and analyze security bugs using Naive Bayes and TF-IDF: A Comparative Analysis},
  Author                   = {Behl, Diksha and Handa, Sahil and Arora, Anuja},
  Year                     = {2014},

  Address                  = {Faridabad, Haryana, India},
  Note                     = {Bug;Bug reports;Bug tracking system;Comparative analysis;Statistical modeling;Term frequencyinverse document frequency (TF-IDF);Text analysis;TF-IDF;},
  Pages                    = {294 - 299},

  Abstract                 = {Bug report contains a vital role during software development, However bug reports belongs to different categories such as performance, usability, security etc. This paper focuses on security bug and presents a bug mining system for the identification of security and non-security bugs using the term frequency-inverse document frequency (TF-IDF) weights and nai&die;ve bayes. We performed experiments on bug report repositories of bug tracking systems such as bugzilla and debugger. In the proposed approach we apply text mining methodology and TF-IDF on the existing historic bug report database based on the bug s description to predict the nature of the bug and to train a statistical model for manually mislabeled bug reports present in the database. The tool helps in deciding the priorities of the incoming bugs depending on the category of the bugs i.e. whether it is a security bug report or a non-security bug report, using nai&die;ve bayes. Our evaluation shows that our tool using TF-IDF is giving better results than the nai&die;ve bayes method. &copy; 2014 IEEE.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  Journal                  = {ICROIT 2014 - Proceedings of the 2014 International Conference on Reliability, Optimization and Information Technology},
  Key                      = {Tools},
  Keywords                 = {Data mining;Information technology;Mining;Optimization;Text processing;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/ICROIT.2014.6798341}
}

@InProceedings{20101512832492,
  Title                    = {What makes a good bug report?},
  Author                   = {Bettenburg, Nicolas and Just, Sascha and Schroter, Adrian and Weiss, Cathrin and Premraj, Rahul and Zimmermann, Thomas},
  Year                     = {2008},

  Address                  = {Atlanta, GA, United states},
  Note                     = {Bug reports;Bug tracking;Mozilla;Software development;Test case;},
  Pages                    = {308 - 318},

  Abstract                 = {In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are at the same time most difficult to provide for users. Such insight is helpful to design new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. In our experiments, CUEZILLA was able to predict the quality of 31 - 48% of bug reports accurately. &copy; 2008 ACM.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  Journal                  = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
  Key                      = {Program debugging},
  Keywords                 = {Computer software selection and evaluation;Software engineering;Surveys;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1145/1453101.1453146}
}

@InProceedings{20125015801011,
  Title                    = {Determining Bug severity using machine learning techniques},
  Author                   = {Chaturvedi, K.K. and Singh, V.B.},
  Year                     = {2012},

  Address                  = {Indore, Madhya Pradesh, India},
  Note                     = {Bug reports;Bug Severity;Bug tracking system;Cross validation;Integral part;K-nearest neighbors;Machine learning techniques;Multinomials;Performance measure;Software bug;Software development process;Software repositories;Supervised classification;Supervised machine learning;},

  Abstract                 = {Software Bug reporting is an integral part of software development process. Once the Bug is reported on Bug Tracking System, their attributes are analyzed and subsequently assigned to various fixers for their resolution. During the last two decades Machine-Learning Techniques (MLT) has been used to create self-improving software. Supervised machine learning technique is widely used for prediction of patterns in various applications but, we have found very few for software repositories. Bug severity, an attribute of a software bug report is the degree of impact that a defect has on the development or operation of a component or system. Bug severity can be classified into different levels based on their impact on the system. In this paper, an attempt has been made to demonstrate the applicability of machine learning algorithms namely Nai&die;ve Bayes, k-Nearest Neighbor, Nai&die;ve Bayes Multinomial, Support Vector Machine, J48 and RIPPER in determining the class of bug severity of bug report data of NASA from PROMISE repository. The applicability of algorithm in determining the various levels of bug severity for bug repositories has been validated using various performance measures by applying 5-fold cross validation<sup>1</sup>. &copy; 2012 IEEE.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  Journal                  = {2012 CSI 6th International Conference on Software Engineering, CONSEG 2012},
  Key                      = {Learning systems},
  Keywords                 = {Feature extraction;Learning algorithms;NASA;Software engineering;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/CONSEG.2012.6349519}
}

@Article{20111313877738,
  Title                    = {An approach to improving bug assignment with bug tossing graphs and bug similarities},
  Author                   = {Chen, Liguo and Wang, Xiaobo and Liu, Chao},
  Journal                  = {Journal of Software},
  Year                     = {2011},
  Note                     = {Bug assignment;Bug reports;Bug tossing;Error prones;Mozilla;Open-source software development;Textual similarities;},
  Number                   = {3},
  Pages                    = {421 - 427},
  Volume                   = {6},

  Abstract                 = {In open-source software development a new bug firstly is found by developers or users. Then the bug is described as a bug report, which is submitted to a bug repository. Finally the bug triager checks the bug report and typically assigns a developer to fix the bug. The assignment process is time-consuming and error-prone. Furthermore, a large number of bug reports are tossed (reassigned) to other developers, which increases bug-fix time. In order to quickly identify the fixer to bug reports we present an approach based on the bug tossing history and textual similarities between bug reports. This proposed approach is evaluated on Eclipse and Mozilla. The results show that our approach can significantly improve the efficiency of bug assignment: the bug fixer is often identified with fewer tossing events. &copy; 2011 ACADEMY PUBLISHER.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {1796217X},
  Key                      = {Program debugging},
  Keywords                 = {Information retrieval;Natural language processing systems;Search engines;Software design;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.4304/jsw.6.3.421-427}
}

@InProceedings{20112714119263,
  Title                    = {Recovering traceability links between source code and fixed bugs via patch analysis},
  Author                   = {Corley, Christopher S. and Kraft, Nicholas A. and Etzkorn, Letha H. and Lukins, Stacy K.},
  Year                     = {2011},

  Address                  = {Waikiki, Honolulu, HI, United states},
  Note                     = {bug assignment;bug mapping;Link recoveries;mining software repositories;trace automation;traceability;},
  Pages                    = {31 - 37},

  Abstract                 = {Traceability links can be recovered using data mined from a revision control system, such as CVS, and an issue tracking system, such as Bugzilla. Existing approaches to recover links between a bug and the methods changed to fix the bug rely on the presence of the bug's identifier in a CVS log message. In this paper we present an approach that relies instead on the presence of a patch in the issue report for the bug. That is, rather than analyzing deltas retrieved from CVS to recover links, our approach analyzes patches retrieved from Bugzilla. We use BugTrace, the tool implementing our approach, to conduct a case study in which we compare the links recovered by our approach to links recovered by manual inspection. The results of the case study support the efficacy of our approach. After describing the limitations of our case study, we conclude by reviewing closely related work and suggesting possible future work. &copy; 2011 ACM.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {02705257},
  Journal                  = {Proceedings - International Conference on Software Engineering},
  Key                      = {Recovery},
  Keywords                 = {Program debugging;Research;Software engineering;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1145/1987856.1987863}
}

@InProceedings{20152801026376,
  Title                    = {Samekana: A Browser Extension for including relevant web links in issue tracking system discussion forum},
  Author                   = {Correa, Denzil and Lal, Sangeeta and Saini, Apoorv and Sureka, Ashish},
  Year                     = {2013},

  Address                  = {Bangkok, Thailand},
  Note                     = {Bug reports;Characterization studies;Data characterization;Empirical Software Engineering;Mining software repository (MSR);Productivity tools;Threaded discussions;Version control system;},
  Pages                    = {25 - 33},
  Volume                   = {1},

  Abstract                 = {Several widely used Issue tracking systems (such as Google Issue Tracker and Bugzilla) contains an integrated threaded discussion forum to facilitate discussion between the development and maintenance team (bug reporters, bug triagers, bug fixers and quality assurance managers). We observe that several comments (and even bug report descriptions) posted to issue tracing system contains links to external websites as references to knowledge sources relevant to the discussion. We conduct a survey (and present the results of the survey) of Google Chromium Developers on the importance and usefulness of web references in issue tracking system comments and the need of a web-browser extension which facilitates easy organization and inclusion of web-links in the post. We conduct a characterization study on an experimental dataset from Google Chromium Issue Tracking system and present results on the distribution of number of links in the dataset, categorization of links into predefined classes (such as blogs, community based Q&amp;A websites, developer discussion forums, version control system), correlation of number and types of links with various bug report types (such as security, crash, regression and clean-up) and relation between presence of links and bug resolution time. Survey results and data characterization study motivate the need of building a developer productivity tool to facilitate web-link (as references) organization and inclusion in issue tracking system comments. We present a Google ChromiumWeb Browser Extension called as Samekana and publish the extension on Google Chromium Web Store which can be freely downloaded by users worldwide. The extension contains features such as annotating (using tags, title and description) and saving web references pertaining to multiple bug reports and tasks and then posting it as bibliography (for easy citation and reference) in issue tracking system comments. &copy; 2013 IEEE.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {15301362},
  Journal                  = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
  Key                      = {Social networking (online)},
  Keywords                 = {Chromium;Computer software maintenance;Human resource management;Productivity;Program debugging;Quality assurance;Software engineering;Surveys;Tracking (position);Websites;World Wide Web;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/APSEC.2013.15}
}

@Article{20162002376318,
  Title                    = {An Automated Approach for Scheduling Bug Fix Tasks},
  Author                   = {De Castro Netto, Fernando and De Oliveira Barros, Marcio and Alvim, Adriana C.F.},
  Journal                  = {International Journal of Software Engineering and Knowledge Engineering},
  Year                     = {2016},
  Note                     = {Automated approach;Bug tracking system;Project managers;Search procedures;Software defects;Software organization;Software project management;Tasks scheduling;},
  Number                   = {2},
  Pages                    = {239 - 271},
  Volume                   = {26},

  Abstract                 = {Software projects usually maintain bug repositories where both developers and end users can report and track the resolution of software defects. These defects should be fixed and new versions of the software incorporating the patches that solve them must be released. The project manager must schedule a set of error correction tasks with different priorities in order to minimize the time required to accomplish these tasks and guarantee that the more important issues have been fixed. This problem is recurrent for most software organizations and, given the enormous number of potential schedules, a tool that searches for good schedules may be helpful to project managers. In this work we propose a genetic algorithm using information captured from bug repositories to find near optimal schedules. We evaluated our approach using a subset of the Eclipse bug repository and the results suggested better schedules than the schedule followed by the developers and schedules proposed by a simpler search procedure. &copy; 2016 World Scientific Publishing Company.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {02181940},
  Key                      = {Scheduling},
  Keywords                 = {Computer software maintenance;Defects;Error correction;Genetic algorithms;Managers;Project management;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1142/S021819401650011X}
}

@InProceedings{20131516195049,
  Title                    = {Practical duplicate bug reports detection in a large web-based development community},
  Author                   = {Feng, Liang and Song, Leyi and Sha, Chaofeng and Gong, Xueqing},
  Year                     = {2013},

  Address                  = {Sydney, NSW, Australia},
  Note                     = {Bug reports;Bug tracking system;Detection performance;Development community;Duplicate bug reports;Duplicate detection;Potential conflict;Waste of resources;},
  Pages                    = {709 - 720},
  Volume                   = {7808 LNCS},

  Abstract                 = {Most of large web-based development communities require a bug tracking system to keep track of various bug reports. However, duplicate bug reports tend to result in waste of resources, and may cause potential conflicts. There have been two types of works focusing on this problem: relevant bug report retrieval [8][11][10][13] and duplicate bug report identification [5][12]. The former methods can achieve high accuracy (82%) in the top 10 results in some dataset, but they do not really reduce the workload of developers. The latter methods still need further improvement on the performance. In this paper, we propose a practical duplicate bug reports detection method, which aims to help project team to reduce their workload by combining existing two categories of methods. We also propose some new features extracted from comments, user profiles and query feedback, which are useful for improving the detection performance. Experiments on real dataset show that our method improves the accuracy rate by 23% compared to state-of-the-art work in duplicate bug report identification, and improves the recall rate by up to 8% in relevant bug report retrieval. &copy; 2013 Springer-Verlag.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {03029743},
  Journal                  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  Key                      = {Websites},
  Keywords                 = {Artificial intelligence;Classification (of information);},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-37401-2_69}
}

@InProceedings{Ghezzi2011,
  Title                    = {SOFAS: A lightweight architecture for software analysis as a service},
  Author                   = {Ghezzi, Giacomo and Gall, Harald C.},
  Year                     = {2011},

  Address                  = {Boulder, CO, United states},
  Note                     = {Bug-fixing;Code duplication;Collaborative softwares;Implementation aspects;Interoperations;Lightweight architecture;Mailing lists;Project stakeholders;Quality analysis;RESTful Web services;Software analysis;Software developer;Software repositories;Software systems;Source code analysis;Version control;},
  Pages                    = {93 - 102},

  __markedentry            = {[vagner:6]},
  Abstract                 = {Access to data stored in software repositories by systems such as version control, bug and issue tracking, or mailing lists is essential for assessing the quality of a software system. A myriad of analyses exploiting that data have been proposed throughout the years: source code analysis, code duplication analysis, co-change analysis, bug prediction, or detection of bug fixing patterns. However, easy and straight forward synergies between these analyses rarely exist. To tackle this problem we have developed SOFA S, a distributed and collaborative software analysis platform to enable a seamless interoperation of such analyses. In particular, software analyses are offered as RESTful web services that can be accessed and composed over the Internet. SOFA S services are accessible through a software analysis catalog where any project stakeholder can, depending on the needs or interests, pick specific analyses, combine them, let them run remotely and then fetch the final results. That way, software developers, testers, architects, or quality assurance experts are given access to quality analysis services. They are shielded from many peculiarities of tool installations and configurations, but SOFA S offers them sophisticated and easy-to-use analyses. This paper describes in detail our SOFAS architecture, its considerations and implementation aspects, and the current set of implemented and offered RESTful analysis services. &copy; 2011 IEEE.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  Journal                  = {Proceedings - 9th Working IEEE/IFIP Conference on Software Architecture, WICSA 2011},
  Key                      = {Quality control},
  Keywords                 = {Computer software selection and evaluation;Groupware;Quality assurance;Software architecture;User interfaces;Web services;},
  Language                 = {English},
  Owner                    = {vagner},
  Timestamp                = {2016.06.07},
  Url                      = {http://dx.doi.org/10.1109/WICSA.2011.21}
}

@InProceedings{20155201729651,
  Title                    = {Process cube for software defect resolution},
  Author                   = {Gupta, Monika and Sureka, Ashish},
  Year                     = {2014},

  Address                  = {Jeju Island, Korea, Republic of},
  Note                     = {Empirical Software Engineering;Issue Tracking;Mining software repositories;OLAP;Peer code review;Process mining;Version control system;},
  Pages                    = {239 - 246},
  Volume                   = {1},

  Abstract                 = {Online Analytical Processing (OLAP) cube is a multi-dimensional dataset used for analyzing data in a Data Warehouse (DW) for the purpose of extracting actionable intelligence. Process mining consists of analyzing event log data produced from Process Aware Information Systems (PAIS) for the purpose of discovering and improving business processes. Process cube is a concept which falls at the intersection of OLAP cube and process mining. Process cube facilitates process mining from multiple-dimensions and enables comparison of process mining results across various dimensions. We present an application of process cube to software defect resolution process to analyze and compare process data from a multi-dimensional perspective. We present a framework, a novel perspective to mine software repositories using process cube. Each cell of process cube is defined by metrics from multiple process mining perspectives like control flow, time, conformance and organizational perspective. We conduct a case-study on Google Chromium project data in which the software defect resolution process spans three software repositories: Issue Tracking System (ITS), Peer Code Review System (PCR) and Version Control System (VCS). We define process cube with 9 dimensions as issue report timestamp, priority, state, closed status, OS, component, bug type, reporter and owner. We define hierarchies along various dimensions and cluster members to handle sparsity. We apply OLAP cube operations such as slice, dice, roll-up and drill-down, and create materialized sublog for each cell. We demonstrate the solution approach by discovering process map and compare process mining results from control flow and time perspective for Performance and Security issues. &copy; 2014 IEEE.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {15301362},
  Journal                  = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
  Key                      = {Process control},
  Keywords                 = {Application programs;Codes (symbols);Control systems;Data mining;Data warehouses;Defects;Geometry;Information management;Software engineering;Tracking (position);},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/APSEC.2014.45}
}

@InProceedings{20131116105559,
  Title                    = {Comparison of seven bug report types: A case-study of Google chrome browser project},
  Author                   = {Lal, Sangeeta and Sureka, Ashish},
  Year                     = {2012},

  Address                  = {Hong Kong, China},
  Note                     = {Bug reports;Characterization studies;Issue Tracking;Mining software repositories;Open source projects;Performance characteristics;Software maintenance process;Tools and practices;},
  Pages                    = {517 - 526},
  Volume                   = {1},

  Abstract                 = {Bug reports submitted to an issue tracking system can belong to different categories such as crash, regression, security, cleanup, polish, performance and usability. A deeper understanding of the properties and features of various categories of bug reports can have implications in improving software maintenance processes, tools and practices. We identify several metrics and characteristics serving as dimensions on which various types of bug reports can be compared. We perform a case-study on Google Chromium Browser open-source project and conduct a series of experiments to calculate various metrics. We present a characterization study comparing different types of bug reports on metrics such as: statistics on close-time, number of stars, number of comments, discriminatory and frequent words for each class, entropy across reporters, entropy across component, opening and closing trend, continuity and debugging efficiency performance characteristics. The calculated metrics shows the similarities and differences on various dimensions for seven different types of bug reports. &copy; 2012 IEEE.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {15301362},
  Journal                  = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
  Key                      = {Software engineering},
  Keywords                 = {Computer software maintenance;Entropy;Tracking (position);},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/APSEC.2012.54}
}

@InProceedings{20123415364468,
  Title                    = {Towards improving bug tracking systems with game mechanisms},
  Author                   = {Lotufo, Rafael and Passos, Leonardo and Czarnecki, Krzysztof},
  Year                     = {2012},

  Address                  = {Zurich, Switzerland},
  Note                     = {Bug reports;Bug tracking system;Online communities;Open-source;Reputation systems;Stack overflow;},
  Pages                    = {2 - 11},

  Abstract                 = {Low bug report quality and human conflicts pose challenges to keep bug tracking systems productive. This work proposes to address these issues by applying game mechanisms to bug tracking systems. We investigate the use of game mechanisms in Stack Overflow, an online community organized to resolve computer programming related problems, for which the improvements we seek for bug tracking systems also turn out to be relevant. The results of our Stack Overflow investigation show that its game mechanisms could be used to address these issues by motivating contributors to increase contribution frequency and quality, by filtering useful contributions, and by creating an agile and dependable moderation system. We proceed by mapping these mechanisms to open-source bug tracking systems, and find that most benefits are applicable. Additionally, our results motivate tailoring a reward and reputation system and summarizing bug reports as future directions for increasing the benefits of game mechanisms in bug tracking systems. &copy; 2012 IEEE.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {21601852},
  Journal                  = {IEEE International Working Conference on Mining Software Repositories},
  Key                      = {Tracking (position)},
  Keywords                 = {Computer programming;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/MSR.2012.6224293}
}

@InProceedings{20153201154952,
  Title                    = {Do developers feel emotions? An exploratory analysis of emotions in software artifacts},
  Author                   = {Murgia, Alessandro and Tourani, Parastou and Adams, Bram and Ortu, Marco},
  Year                     = {2014},

  Address                  = {Hyderabad, India},
  Note                     = {Apache software foundations;Collaborative activities;Complex software systems;Emotional information;Empirical Software Engineering;Exploratory analysis;Issue report;Software artifacts;},
  Pages                    = {262 - 271},

  Abstract                 = {Software development is a collaborative activity in which developers interact to create and maintain a complex software system. Human collaboration inevitably evokes emotions like joy or sadness, which can affect the collaboration either positively or negatively, yet not much is known about the individual emotions and their role for software development stakeholders. In this study, we analyze whether development artifacts like issue reports carry any emotional information about software development. This is a first step towards verifying the feasibility of an automatic tool for emotion mining in software development artifacts: if humans cannot determine any emotion from a software artifact, neither can a tool. Analysis of the Apache Software Foundation issue tracking system shows that developers do express emotions (in particular gratitude, joy and sadness). However, the more context is provided about an issue report, the more human raters start to doubt and nuance their interpretation of emotions. More investigation is needed before building a fully automatic emotion mining tool. Copyright is held by the author/owner(s). Publication rights licensed to ACM.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  Journal                  = {11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings},
  Key                      = {Software design},
  Keywords                 = {Software engineering;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1145/2597073.2597086}
}

@InProceedings{20110913711739,
  Title                    = {An automated approach for scheduling bug fix tasks},
  Author                   = {Netto, Fernando and Barros, Marcio and Alvim, Adriana C. F.},
  Year                     = {2010},

  Address                  = {Salvador, Bahia, Brazil},
  Note                     = {Automated approach;Development teams;End users;High-quality software;Project managers;Software development life cycle;Software engineering practices;Task schedule;},
  Pages                    = {80 - 89},

  Abstract                 = {Even if a development team uses the best Software Engineering practices to produce high-quality software, end users may find defects that were not previously identified during the software development life-cycle. These defects must be fixed and new versions of the software incorporating the patches that solve them must be released. The project manager must schedule a set of error correction tasks with different priorities in order to minimize the time required to accomplish these tasks and guarantee that the more important issues have been fixed. Given the large number of distinct schedules, an automatically tool to find good schedules may be helpful to project managers. This work proposes a method which captures relevant information from bug repositories and submits them to a genetic algorithm to find near optimal bug correction task schedules. We have evaluated the approach using a subset of the Eclipse bug repository and it suggested better schedules than the actual schedules followed by Eclipse developers. &copy; 2010 IEEE.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  Journal                  = {Proceedings - 24th Brazilian Symposium on Software Engineering, SBES 2010},
  Key                      = {Software design},
  Keywords                 = {Computer aided software engineering;Defects;Managers;Project management;},
  Language                 = {Portuguese},
  Url                      = {http://dx.doi.org/10.1109/SBES.2010.16}
}

@InProceedings{20114214440528,
  Title                    = {Detecting bug duplicate reports through local references},
  Author                   = {Prifti, Tomi and Banerjee, Sean and Cukic, Bojan},
  Year                     = {2011},

  Address                  = {Banff, AB, Canada},
  Note                     = {Bug reports;Bug tacking systems;Bug tracking;Detection rates;Firefox;Long duration;Open source projects;Runtimes;Search tools;Side effect;Software quality;},

  Abstract                 = {Background: Bug Tracking Repositories, such as Bugzilla, are designed to support fault reporting for developers, testers and users of the system. Allowing anyone to contribute finding and reporting faults has an immediate impact on software quality. However, this benefit comes with at least one side-effect. Users often file reports that describe the same fault. This increases the maintainer's triage time, but important information required to fix the fault is likely contributed by different reports. Aim: The objective of this paper is twofold. First, we want to understand the dynamics of bug report filing for a large, long duration open source project, Firefox. Second, we present a new approach that can reduce the number of duplicate reports. Method: The novel element in the proposed approach is the ability to concentrate the search for duplicates on specific portions of the bug repository. Our system can be deployed as a search tool to help reporters query the repository. Results: When tested as a search tool our system is able to detect up to 53% of duplicate reports. Conclusion: The performance of Information Retrieval techniques can be significantly improved by guiding the search for duplicates. This approach results in higher detection rates and constant classification runtime. Copyright &copy; 2011 ACM.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  Journal                  = {ACM International Conference Proceeding Series},
  Key                      = {Computer software maintenance},
  Keywords                 = {Computer software selection and evaluation;Information retrieval;Models;Predictive control systems;Software engineering;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1145/2020390.2020398}
}

@InProceedings{20102913088730,
  Title                    = {JDF: Detecting duplicate bug reports in Jazz},
  Author                   = {Song, Yoonki and Wang, Xiaoyin and Xie, Tao and Zhang, Lu and Mei, Hong},
  Year                     = {2010},

  Address                  = {Cape Town, South africa},
  Note                     = {Bug reports;Information information;Natural languages;Process management;Software development;Software Quality;Team collaboration;},
  Pages                    = {315 - 316},
  Volume                   = {2},

  Abstract                 = {Both developers and users submit bug reports to a bug repository. These reports can help reveal defects and improve software quality. As the number of bug reports in a bug repository increases, the number of the potential duplicate bug reports increases. Detecting duplicate bug reports helps reduce development efforts in fixing defects. However, it is challenging to manually detect all potential duplicates because of the large number of existing bug reports. This paper presents JDF (representing Jazz Duplicate Finder), a tool that helps users to find potential duplicates of bug reports on Jazz, which is a team collaboration platform for software development and process management. JDF finds potential duplicates for a given bug report using natural language and execution information. &copy; 2010 ACM.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {02705257},
  Journal                  = {Proceedings - International Conference on Software Engineering},
  Key                      = {Program debugging},
  Keywords                 = {Computer software selection and evaluation;Defects;Information retrieval;Software design;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1145/1810295.1810368}
}

@InProceedings{20144600189216,
  Title                    = {DupFinder: Integrated tool support for duplicate bug report detection},
  Author                   = {Thung, Ferdian and Kochhar, Pavneet Singh and Lo, David},
  Year                     = {2014},

  Address                  = {Vasteras, Sweden},
  Note                     = {Automated approach;Bug tracking system;Bugzilla;Distributed process;Duplicate bug reports;Integrated tools;Open source tools;Vector space models;},
  Pages                    = {871 - 874},

  Abstract                 = {To track bugs that appear in a software, developers often make use of a bug tracking system. Users can report bugs that they encounter in such a system. Bug reporting is inherently an uncoordinated distributed process though and thus when a user submits a new bug report, there might be cases when another bug report describing exactly the same problem is already present in the system. Such bug reports are duplicate of each other and these duplicate bug reports need to be identified. A number of past studies have proposed a number of automated approaches to detect duplicate bug reports. However, these approaches are not integrated to existing bug tracking systems. In this paper, we propose a tool named DupFinder, which implements the state-of-theart unsupervised duplicate bug report approach by Runeson et al., as a Bugzilla extension. DupFinder does not require any training data and thus can easily be deployed to any project. DupFinder extracts texts from summary and description fields of a new bug report and recent bug reports present in a bug tracking system, uses vector space model to measure similarity of bug reports, and provides developers with a list of potential duplicate bug reports based on the similarity of these reports with the new bug report. We have released DupFinder as an open source tool in GitHub, which is available at: uhttps://github.com/smagsmu/dupfinder. &copy; 2014 ACM.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  Journal                  = {ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
  Key                      = {Program debugging},
  Keywords                 = {Open source software;Software engineering;Tracking (position);Vector spaces;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1145/2642937.2648627}
}

@InProceedings{20121915000192,
  Title                    = {Improved duplicate bug report identification},
  Author                   = {Tian, Yuan and Sun, Chengnian and Lo, David},
  Year                     = {2012},

  Address                  = {Szeged, Hungary},
  Note                     = {Bug reports;Bug tracking system;Bugzilla;Direct detection;End users;Management systems;Mozilla;Relative similarity;Software systems;},
  Pages                    = {385 - 390},

  Abstract                 = {Bugs are prevalent in software systems. To improve the reliability of software systems, developers often allow end users to provide feedback on bugs that they encounter. Users could perform this by sending a bug report in a bug report management system like Bugzilla. This process however is uncoordinated and distributed, which means that many users could submit bug reports reporting the same problem. These are referred to as duplicate bug reports. The existence of many duplicate bug reports may cause much unnecessary manual efforts as often a triager would need to manually tag bug reports as being duplicates. Recently, there have been a number of studies that investigate duplicate bug report problem which in effect answer the following question: given a new bug report, retrieve k other similar bug reports. This, however, still requires substantive manual effort which could be reduced further. Jalbert and Weimer are the first to introduce the direct detection of duplicate bug reports; it answers the question: given a new bug report, classify if it as a duplicate bug report or not. In this paper, we extend Jalbert and Weimer's work by improving the accuracy of automated duplicate bug report identification. We experiments with bug reports from Mozilla bug tracking system which were reported between February 2005 to October 2005, and find that we could improve the accuracy of the previous approach by about 160%. &copy; 2012 IEEE.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {15345351},
  Journal                  = {Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR},
  Key                      = {Computer software maintenance},
  Keywords                 = {Computer software;Reengineering;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/CSMR.2012.48}
}

@InProceedings{20112614089813,
  Title                    = {Which bug should I fix: Helping new developers onboard a new project},
  Author                   = {Wang, Jianguo and Sarma, Anita},
  Year                     = {2011},

  Address                  = {Waikiki, Honolulu, HI, United states},
  Note                     = {bug search;Entry point;new developer;New projects;Open source projects;Search capabilities;Sociotechnical;Tesseract;},
  Pages                    = {76 - 79},

  Abstract                 = {A typical entry point for new developers in an open source project is to contribute a bug fix. However, finding an appropriate bug and an appropriate fix for that bug requires a good understanding of the project, which is nontrivial. Here, we extend Tesseract - an interactive project exploration environment - to allow new developers to search over bug descriptions in a project to quickly identify and explore bugs of interest and their related resources. More specifically, we extended Tesseract with search capabilities that enable synonyms and similar-bugs search over bug descriptions in a bug repository. The goal is to enable users to identify bugs of interest, resources related to that bug, (e.g., related files, contributing developers, communication records), and visually explore the appropriate socio-technical dependencies for the selected bug in an interactive manner. Here we present our search extension to Tesseract. &copy; 2011 ACM.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {02705257},
  Journal                  = {Proceedings - International Conference on Software Engineering},
  Key                      = {Open systems},
  Keywords                 = {Software engineering;User interfaces;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1145/1984642.1984661}
}

@InProceedings{20120214674950,
  Title                    = {BugMiner: Software reliability analysis via data mining of bug reports},
  Author                   = {Wu, Leon and Xie, Boyi and Kaiser, Gail and Passonneau, Rebecca},
  Year                     = {2011},

  Address                  = {Miami, FL, United states},
  Note                     = {Bug reports;Bug tracking;Empirical studies;Human users;Low qualities;Software bug;},
  Pages                    = {95 - 100},

  Abstract                 = {Software bugs reported by human users and automatic error reporting software are often stored in some bug tracking tools (e.g., Bugzilia and Debbugs). These accumulated bug reports may contain valuable information that could be used to improve the quality of the bug reporting, reduce the quality assurance effort and cost, analyze software reliability, and predict future bug report trend. In this paper, we present BugMiner, a tool that is able to derive useful information from historic bug report database using data mining, use these information to do completion check and redundancy check on a new or given bug report, and to estimate the bug report trend using statistical analysis. Our empirical studies of the tool using several real-world bug report repositories show that it is effective, easy to implement, and has relatively high accuracy despite low quality data.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  Journal                  = {SEKE 2011 - Proceedings of the 23rd International Conference on Software Engineering and Knowledge Engineering},
  Key                      = {Software reliability},
  Keywords                 = {Computer software selection and evaluation;Data mining;Information use;Knowledge engineering;Program debugging;Quality assurance;Reliability analysis;Software engineering;},
  Language                 = {English}
}

@Article{20104113287097,
  Title                    = {What makes a good bug report?},
  Author                   = {Zimmermann, Thomas and Premraj, Rahul and Bettenburg, Nicolas and Just, Sascha and Schroter, Adrian and Weiss, Cathrin},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2010},
  Note                     = {and enhancement;Bug reports;Bug tracking;Bug tracking system;distribution;human factors;Mozilla;Software development;Test case;Testing and debugging;Tool support;},
  Number                   = {5},
  Pages                    = {618 - 643},
  Volume                   = {36},

  Abstract                 = {In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are, at the same time, most difficult to provide for users. Such insight is helpful for designing new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. The participants of our survey also provided 175 comments on hurdles in reporting and resolving bugs. Based on these comments, we discuss several recommendations for better bug tracking systems, which should focus on engaging bug reporters, better tool support, and improved handling of bug duplicates. &copy; 2010 IEEE.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {00985589},
  Key                      = {Program debugging},
  Keywords                 = {Human engineering;Maintenance;Software design;Surveys;},
  Language                 = {English},
  Url                      = {http://dx.doi.org/10.1109/TSE.2010.63}
}

@InProceedings{20123415364483,
  Title                    = {2012 9th IEEE Working Conference on Mining Software Repositories, MSR 2012 - Proceedings},
  Year                     = {2012},

  Address                  = {Zurich, Switzerland},

  Abstract                 = {The proceedings contain 38 papers. The topics discussed include: towards improving bug tracking systems with game mechanisms; a linked data platform for mining software repositories; how distributed version control systems impact open source software projects; an empirical study of supplementary bug fixes; incorporating version histories in information retrieval based bug localization; think locally, act globally: improving defect and effort prediction models; are faults localizable?; green mining: a methodology of relating software change to power consumption; analysis of customer satisfaction survey data; mining usage data and development artifacts; bug introducing changes: a case study with android; trendy bugs: topic trends in the android bug reports; do the stars align? multidimensional analysis of android's layered architecture; and the build dependency perspective of android's concrete architecture.},
  Copyright                = {Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
  ISSN                     = {21601852},
  Journal                  = {IEEE International Working Conference on Mining Software Repositories},
  Language                 = {English}
}

